{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing and Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_filepath = '/sietch_colab/akapoor/Demographic_Inference'\n",
    "\n",
    "\n",
    "\n",
    "CONFIG_FILEPATH = f'{main_filepath}/experiment_config.json'\n",
    "MODEL_CONFIG_FILEPATH = f'{main_filepath}/model_config.json'\n",
    "\n",
    "with open(CONFIG_FILEPATH, 'r') as f:\n",
    "   experiment_config = json.load(f)\n",
    "\n",
    "with open(MODEL_CONFIG_FILEPATH, 'r') as f:\n",
    "   model_config = json.load(f)\n",
    "\n",
    "\n",
    "CWD = os.getcwd()\n",
    "\n",
    "# Use double quotes for the dictionary keys inside the f-string\n",
    "EXPERIMENT_DIRECTORY = f\"{experiment_config['demographic_model']}_dadi_analysis_{experiment_config['dadi_analysis']}_moments_analysis_{experiment_config['moments_analysis']}_momentsLD_analysis_{experiment_config['momentsLD_analysis']}_seed_{experiment_config['seed']}\"\n",
    "EXPERIMENT_NAME = f'sims_pretrain_{experiment_config[\"num_sims_pretrain\"]}_sims_inference_{experiment_config[\"num_sims_inference\"]}_seed_{experiment_config[\"seed\"]}_num_replicates_{experiment_config[\"k\"]}_top_values_{experiment_config[\"top_values_k\"]}'\n",
    "SIM_DIRECTORY = f\"{EXPERIMENT_DIRECTORY}/sims/{EXPERIMENT_NAME}\"\n",
    "\n",
    "# Check if hidden_size is a list, and if so, join the elements with \"_\"\n",
    "hidden_size = model_config['neural_net_hyperparameters']['hidden_size']\n",
    "if isinstance(hidden_size, list):\n",
    "    hidden_size_str = \"_\".join(map(str, hidden_size))  # Join list elements with \"_\"\n",
    "else:\n",
    "    hidden_size_str = str(hidden_size)  # Convert integer to string if not a list\n",
    "\n",
    "# Build the MODEL_DIRECTORY string\n",
    "MODEL_DIRECTORY = (\n",
    "    f\"{EXPERIMENT_DIRECTORY}/models/{EXPERIMENT_NAME}/\"\n",
    "    f\"num_hidden_neurons_{hidden_size_str}_\"\n",
    "    f\"num_hidden_layers_{model_config['neural_net_hyperparameters']['num_layers']}_\"\n",
    "    f\"num_epochs_{model_config['neural_net_hyperparameters']['num_epochs']}_\"\n",
    "    f\"dropout_value_{model_config['neural_net_hyperparameters']['dropout_rate']}_\"\n",
    "    f\"weight_decay_{model_config['neural_net_hyperparameters']['weight_decay']}_\"\n",
    "    f\"batch_size_{model_config['neural_net_hyperparameters']['batch_size']}_\"\n",
    "    f\"EarlyStopping_{model_config['neural_net_hyperparameters']['EarlyStopping']}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/sietch_colab/akapoor/Demographic_Inference/')\n",
    "os.environ['PYTHONPATH'] = '/sietch_colab/akapoor/Demographic_Inference:' + os.environ.get('PYTHONPATH', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing training data:\n",
      "===> Normalizing the data.\n",
      "\n",
      "Processing validation data:\n",
      "===> Normalizing the data.\n",
      "Postprocessing dict keys: dict_keys(['normalization', 'predictions', 'normalized_predictions', 'targets', 'normalized_targets'])\n",
      "Postprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "!python /sietch_colab/akapoor/Demographic_Inference/snakemake_scripts/postprocessing.py \\\n",
    "    --config_file $CONFIG_FILEPATH \\\n",
    "    --training_features_filepath $SIM_DIRECTORY/training_features.csv \\\n",
    "    --training_targets_filepath $SIM_DIRECTORY/training_targets.csv \\\n",
    "    --validation_features_filepath $SIM_DIRECTORY/validation_features.csv \\\n",
    "    --validation_targets_filepath $SIM_DIRECTORY/validation_targets.csv \\\n",
    "    --sim_directory $SIM_DIRECTORY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['parameter_names', 'target_names', 'training', 'validation'])\n",
      "Training features shape: (375, 40)\n",
      "Validation features shape: (94, 40)\n",
      "Training targets shape: (375, 4)\n",
      "Validation targets shape: (94, 4)\n"
     ]
    }
   ],
   "source": [
    "!python /sietch_colab//akapoor/Demographic_Inference/snakemake_scripts/extracting_features.py \\\n",
    " --postprocessing_results_filepath $SIM_DIRECTORY/postprocessing_results.pkl \\\n",
    " --sim_directory $SIM_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory created/verified: split_isolation_model_dadi_analysis_True_moments_analysis_True_momentsLD_analysis_True_seed_42/models/sims_pretrain_500_sims_inference_1_seed_42_num_replicates_3_top_values_2/num_hidden_neurons_10_num_hidden_layers_2_num_epochs_500_dropout_value_0_weight_decay_0_batch_size_64_EarlyStopping_False\n",
      "Initializing LinearRegression with kwargs={}\n",
      "PREDICTIONS SHAPE TRAINING: (375, 4)\n",
      "[2.17040379e-04 1.60429237e-04 1.06441734e-04 1.50368701e-03\n",
      " 8.26248087e-04 1.89832255e-04 1.24870579e-03 1.76666686e-04\n",
      " 4.68037115e-03 5.99968006e-05 3.64516289e-04 6.07166169e-04\n",
      " 1.99164679e-05 1.89186152e-04 2.24410129e-03 4.44461757e-04\n",
      " 8.30250839e-04 3.20050702e-04 6.67372049e-04 4.36815092e-03\n",
      " 3.40148062e-05 2.65255535e-03 7.39937702e-04 4.93398717e-04\n",
      " 3.78875481e-04 1.62686124e-03 8.04517662e-04 2.97266536e-04\n",
      " 6.15999161e-04 1.40511056e-04 1.53534570e-02 2.21760329e-04\n",
      " 1.80026333e-04 6.05329087e-04 1.14544154e-03 1.49647558e-04\n",
      " 4.27424353e-04 5.11889835e-04 4.53690212e-04 2.61206514e-04\n",
      " 1.54032169e-04 6.48878341e-04 7.58458397e-04 1.02655007e-04\n",
      " 4.10758810e-04 1.05881471e-03 3.13086794e-03 1.07717128e-03\n",
      " 1.24242527e-03 1.62935539e-05 4.89944049e-06 4.34485942e-03\n",
      " 2.77945894e-04 8.79488994e-04 1.34642744e-03 7.50013976e-04\n",
      " 1.33648062e-04 3.34597177e-04 6.08654270e-04 9.49580352e-05\n",
      " 3.29663295e-04 1.19790022e-04 5.18697363e-04 1.68937906e-03\n",
      " 1.97364247e-04 8.27729469e-05 3.73847189e-04 9.59685463e-04\n",
      " 3.25874879e-04 2.06463476e-03 5.36845143e-04 9.78891566e-04\n",
      " 4.12446113e-03 4.26786206e-04 5.39377421e-03 4.60855260e-05\n",
      " 1.23283960e-03 2.31568399e-04 4.80450280e-05 1.75053982e-04\n",
      " 3.00656376e-04 3.13119304e-04 4.48914514e-04 4.19582772e-04\n",
      " 5.73838543e-04 9.87920784e-04 8.77450418e-04 2.64815752e-04\n",
      " 5.39533135e-03 7.68959706e-05 2.37146683e-04 8.11898605e-06\n",
      " 2.49031580e-03 3.35353976e-03 7.90123821e-05 4.53048694e-04\n",
      " 2.15095898e-03 7.32746831e-05 1.82489410e-04 4.57488614e-04\n",
      " 1.74274933e-03 7.69173112e-04 9.75365535e-06 2.50710183e-04\n",
      " 1.46818227e-04 1.80893743e-04 1.26939685e-04 6.27602017e-04\n",
      " 1.09805557e-03 1.60095361e-04 7.84048821e-04 8.43045405e-05\n",
      " 3.57386463e-03 5.32828008e-04 4.99157048e-04 2.02480254e-03\n",
      " 5.17891960e-04 1.08576496e-03 1.03004031e-04 4.85009182e-04\n",
      " 8.78966607e-04 2.23400870e-04 2.84923996e-05 3.63228650e-05\n",
      " 1.94872427e-04 3.53693239e-04 5.07310628e-04 7.69173907e-05\n",
      " 3.08009533e-04 7.61870104e-04 2.89037747e-03 1.84171622e-03\n",
      " 7.22057128e-04 1.52439113e-04 2.30524259e-04 4.73139668e-04\n",
      " 3.21653101e-03 1.63999180e-03 2.33769399e-04 4.37055930e-04\n",
      " 8.33456028e-04 5.11090174e-03 6.25027415e-04 4.71740435e-04\n",
      " 2.48837006e-04 8.55280751e-04 1.84689782e-03 1.67304102e-04\n",
      " 4.53356885e-04 1.20225602e-02 9.80834905e-04 3.05746323e-03\n",
      " 1.62810826e-04 6.34024814e-04 1.12948431e-02 9.00848862e-05\n",
      " 9.21862413e-04 4.26830260e-04 1.43824184e-04 2.24033037e-04\n",
      " 1.06946924e-02 2.51721261e-04 2.89625388e-04 3.55792911e-04\n",
      " 1.47662236e-04 5.18993539e-04 2.08551491e-04 3.50311686e-04\n",
      " 3.75915971e-05 2.16134474e-04 2.35974282e-03 1.73934383e-03\n",
      " 1.01707210e-03 3.22395110e-04 1.00736811e-04 2.27699642e-04\n",
      " 1.85057319e-04 2.73533949e-04 1.31013655e-03 2.10155255e-04\n",
      " 5.10228020e-04 4.69182004e-04 2.44839997e-04 4.87467503e-04\n",
      " 2.16608797e-05 7.97949553e-04 5.03264140e-04 8.68323327e-04\n",
      " 1.93057428e-03 2.31339436e-04 5.55193690e-04 1.50144562e-04\n",
      " 7.97534396e-03 3.84216791e-04 1.05413609e-04 8.38018851e-04\n",
      " 3.57340689e-04 2.04626532e-03 5.65285073e-03 1.25854691e-03\n",
      " 6.84405665e-04 1.82268074e-04 5.31864623e-04 7.66634502e-04\n",
      " 9.34637008e-04 4.24831641e-03 7.13585458e-04 1.87642903e-04\n",
      " 7.98932173e-03 3.23941603e-04 2.54267616e-04 3.47367223e-04\n",
      " 2.33892160e-04 2.12839614e-04 2.36528707e-04 5.10946534e-04\n",
      " 8.50827051e-05 4.34755155e-03 3.29711440e-05 6.41533305e-04\n",
      " 5.40611530e-04 1.35568867e-04 4.85449394e-04 1.21997228e-04\n",
      " 5.34847342e-04 7.69572835e-04 1.57199530e-03 8.79105945e-04\n",
      " 1.30351186e-04 9.75606276e-04 1.69238259e-04 7.11861670e-04\n",
      " 5.48147041e-04 3.01751818e-04 5.34374061e-04 4.39004069e-02\n",
      " 6.10151675e-01 8.44686385e-04 8.02020261e-05 1.82047079e-04\n",
      " 1.01061187e-04 1.30998233e-02 1.29211836e-04 3.38441285e-04\n",
      " 7.85286464e-04 8.73517902e-03 1.07227405e-04 6.21096168e-04\n",
      " 3.09714225e-05 3.60987003e-04 9.81062503e-04 2.65806927e-04\n",
      " 2.61469897e-03 9.44999152e-05 3.82657791e-04 6.80685502e-04\n",
      " 6.32873702e-04 5.32342979e-04 2.11036093e-04 1.43952194e-04\n",
      " 5.32147697e-05 3.61278782e-04 7.42109561e-05 1.57042063e-04\n",
      " 2.19235985e-04 3.20278080e-03 8.76057575e-05 2.15020429e-04\n",
      " 6.01162800e-04 5.39857656e-04 3.67768991e-04 7.62839129e-05\n",
      " 5.12903036e-04 2.38514458e-04 1.53234320e-04 5.17097564e-04\n",
      " 9.90487426e-05 5.35539224e-04 5.48911869e-04 4.08073221e-04\n",
      " 2.25346725e-04 4.37815689e-04 5.92002293e-04 2.89117051e-04\n",
      " 1.73367985e-04 4.33204467e-04 8.38631771e-04 4.33527075e-04\n",
      " 1.41692760e-04 3.23105730e-03 3.36828632e-04 4.50354032e-04\n",
      " 9.48563437e-04 1.29158805e-03 4.90048125e-04 2.15607088e-04\n",
      " 9.86276919e-04 4.61027203e-04 7.96664548e-04 9.17736671e-05\n",
      " 3.61623522e-04 1.57841944e-04 2.80282074e-03 8.04305356e-04\n",
      " 1.26069076e-04 3.52490733e-05 3.49415442e-04 2.22891244e-04\n",
      " 1.06665508e-03 1.97344339e-03 8.37646420e-05 1.08086438e-04\n",
      " 8.70423229e-05 3.30272153e-05 1.84966198e-04 1.38994491e-04\n",
      " 1.65460740e-04 4.75864433e-03 1.36823159e-02 1.04792903e-04\n",
      " 6.32198373e-04 1.35475954e-03 3.29277021e-03 2.90254612e-04\n",
      " 1.16515954e-04 1.10473663e-04 2.30542776e-03 7.24746668e-04\n",
      " 1.18089951e-03 6.44428022e-04 8.87179124e-05 5.85470262e-04\n",
      " 1.42093021e-04 6.05935120e-05 2.07307939e-04 3.97899377e-03\n",
      " 5.32813089e-04 3.02464467e-03 7.88318570e-04 1.17871445e-03\n",
      " 3.34279943e-04 2.77168098e-04 7.26321888e-04 7.08068537e-04\n",
      " 3.58303067e-04 9.08733393e-04 1.57612218e-04 4.54990567e-03\n",
      " 2.97457927e-04 3.91074872e-03 2.89603987e-04 3.93594536e-04\n",
      " 2.68567827e-03 6.95605341e-04 6.52837535e-03 8.00300889e-04\n",
      " 2.51952530e-04 2.99996742e-04 1.63841148e-03 4.12006225e-04\n",
      " 8.71822408e-04 6.33867194e-01 9.69232361e-04 1.15049009e-04\n",
      " 7.20881736e-05 2.10950000e-03 7.48062809e-04 3.19824015e-04\n",
      " 6.49970767e-04 2.87453014e-04 4.32879771e-04 5.83987229e-04\n",
      " 3.72503862e-04 4.49261270e-04 4.36303433e-04]\n",
      "[1.58072479e-04 3.25445297e-04 9.99044752e-04 5.17453842e-04\n",
      " 3.24039511e-04 2.80167272e-04 8.59933872e-05 9.31856253e-04\n",
      " 1.11589540e-04 3.71618300e-04 1.00664849e-03 3.65386548e-04\n",
      " 1.38243447e-04 3.00936317e-03 1.75780199e-04 7.20476536e-05\n",
      " 7.03695132e-04 6.51880109e-05 3.42204384e-04 2.31262331e-05\n",
      " 1.90975031e-03 6.89465527e-01 7.67094756e-04 4.83816828e-04\n",
      " 6.13916407e-03 1.06459727e-03 1.36383627e-04 5.58516292e-04\n",
      " 1.26289189e-03 7.81700117e-04 7.20498133e-04 8.08462081e-03\n",
      " 1.14907905e-03 1.87588592e-04 1.55345659e-03 5.91883621e-04\n",
      " 2.60229028e-04 3.00268461e-04 1.21685277e-04 1.76772913e-03\n",
      " 9.43606340e-05 6.83068712e-05 1.94147718e-04 3.84026377e-04\n",
      " 1.03516598e-03 5.60549999e-03 1.09655234e-04 6.06695530e-04\n",
      " 7.79259581e-04 1.02703140e-04 5.04198782e-04 1.35561870e-04\n",
      " 4.94758743e-04 9.19026638e-04 3.19495731e-03 8.51349626e-04\n",
      " 2.95311661e-04 5.68498319e-04 4.94713436e-04 7.25772015e-04\n",
      " 7.23478793e-04 9.91972846e-05 2.57449766e-05 1.41079110e-04\n",
      " 6.72633950e-04 4.60173929e-04 1.45335015e-03 2.06058251e-04\n",
      " 5.34699425e-04 1.58955385e-03 2.73202482e-04 8.30213534e-04\n",
      " 2.91364555e-04 4.80067781e-04 5.84512219e-04 9.31895087e-04\n",
      " 5.86818183e-03 2.25176165e-05 7.43344165e-04 1.13691822e-03\n",
      " 1.09497250e-03 1.29531182e-03 5.40491787e-04 2.43694140e-04\n",
      " 2.91922641e-03 1.89615078e-03 6.97780073e-05 9.18599464e-05\n",
      " 3.16251281e-03 3.93414789e-04 1.79975329e-04 1.61743320e-03\n",
      " 2.27092423e-04 3.05798796e-04]\n",
      "Figure(1000x1000)\n",
      "Linear model trained LFG\n"
     ]
    }
   ],
   "source": [
    "!python /sietch_colab/akapoor/Demographic_Inference/snakemake_scripts/linear_evaluation.py \\\n",
    "     --features_and_targets_filepath /sietch_colab/akapoor/Demographic_Inference/split_isolation_model_dadi_analysis_True_moments_analysis_True_momentsLD_analysis_True_seed_42/sims/sims_pretrain_500_sims_inference_1_seed_42_num_replicates_3_top_values_2/features_and_targets.pkl \\\n",
    "     --model_config_path /sietch_colab/akapoor/Demographic_Inference/model_config.json \\\n",
    "     --color_shades_file /sietch_colab/akapoor/Demographic_Inference/split_isolation_model_dadi_analysis_True_moments_analysis_True_momentsLD_analysis_True_seed_42/sims/sims_pretrain_500_sims_inference_1_seed_42_num_replicates_3_top_values_2/color_shades.pkl \\\n",
    "     --main_colors_file /sietch_colab/akapoor/Demographic_Inference/split_isolation_model_dadi_analysis_True_moments_analysis_True_momentsLD_analysis_True_seed_42/sims/sims_pretrain_500_sims_inference_1_seed_42_num_replicates_3_top_values_2/main_colors.pkl \\\n",
    "     --experiment_config_filepath /sietch_colab/akapoor/Demographic_Inference/experiment_config.json \\\n",
    "     --regression_type standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory created/verified: split_isolation_model_dadi_analysis_True_moments_analysis_True_momentsLD_analysis_True_seed_42/models/sims_pretrain_500_sims_inference_1_seed_42_num_replicates_3_top_values_2/num_hidden_neurons_1000_num_hidden_layers_2_num_epochs_100_dropout_value_0_weight_decay_0_batch_size_64_EarlyStopping_False\n",
      "\n",
      "No hyperparameters specified. Running RandomizedSearchCV to find best hyperparameters...\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "Best hyperparameters found via RandomizedSearchCV: {'random_state': 2023, 'n_estimators': 50, 'min_samples_split': 5, 'max_depth': 30}\n",
      "\n",
      "Initializing RandomForestRegressor with kwargs={'n_estimators': 50, 'max_depth': 30, 'random_state': 2023, 'min_samples_split': 5}\n",
      "\n",
      "Random Forest predictions shape (training): (375, 4)\n",
      "Random Forest predictions shape (validation): (94, 4)\n",
      "\n",
      "[1.40051926e-04 7.35507236e-05 1.76780185e-04 3.72394739e-04\n",
      " 6.29923147e-04 2.30621027e-04 1.10078211e-05 2.70376461e-04\n",
      " 2.41653069e-04 1.56746871e-05 5.99200741e-05 1.30618427e-04\n",
      " 1.92878697e-04 2.35150816e-04 6.93005844e-05 2.33878399e-04\n",
      " 7.02043227e-04 8.80594350e-03 6.82065960e-05 3.17172086e-04\n",
      " 5.72887355e-05 5.01402384e-04 3.13885892e-04 1.19179040e-04\n",
      " 1.31870451e-04 8.13247677e-04 1.67523669e-04 1.11802012e-05\n",
      " 6.56987967e-05 6.12916689e-05 2.48116928e-03 2.05109840e-04\n",
      " 1.42579849e-04 6.11258063e-05 3.18319355e-05 7.15014655e-05\n",
      " 2.23159079e-04 2.39642315e-05 2.78696308e-05 4.84209233e-05\n",
      " 1.22323263e-04 8.31416711e-05 2.13111448e-04 4.29974402e-03\n",
      " 1.50388766e-04 7.81394720e-05 5.69750805e-04 8.70124459e-05\n",
      " 1.26756976e-03 1.81705927e-04 2.28959713e-04 6.22411030e-05\n",
      " 7.01273759e-05 8.23564632e-05 2.63445597e-05 1.24112265e-05\n",
      " 2.26829090e-04 5.57959502e-05 1.25739697e-04 3.49237071e-05\n",
      " 1.75057132e-05 3.01245882e-04 2.63795412e-05 3.28772607e-05\n",
      " 6.58074188e-05 7.29888471e-05 7.41655133e-05 2.42766243e-04\n",
      " 2.27766971e-05 5.73286574e-05 9.97453332e-05 3.27219239e-04\n",
      " 2.16516684e-04 8.75825708e-05 3.97808995e-04 1.22773041e-05\n",
      " 5.13246227e-04 4.47617627e-04 7.53014514e-05 1.38835488e-04\n",
      " 9.43864017e-05 6.71155620e-04 1.31537234e-04 2.01723472e-06\n",
      " 6.19904944e-05 2.30836717e-04 9.73994938e-05 3.89634317e-05\n",
      " 3.20347955e-04 1.29512479e-04 1.24309324e-04 1.00028678e-04\n",
      " 1.18947174e-04 1.15899161e-04 6.95205702e-05 6.54526520e-05\n",
      " 4.05981852e-04 3.11533534e-05 6.53402821e-05 1.94585844e-05\n",
      " 4.31637567e-04 4.66990400e-05 3.25160485e-04 4.08898624e-05\n",
      " 2.11275775e-04 2.42451920e-04 1.51548284e-04 5.56795499e-05\n",
      " 2.57783983e-04 1.29570809e-04 3.24525443e-05 3.20131891e-05\n",
      " 4.61566816e-04 6.56385465e-05 6.71236397e-05 7.37400884e-05\n",
      " 1.32676208e-04 2.94165819e-05 3.59100891e-05 4.16606809e-05\n",
      " 1.52276313e-04 2.52306829e-03 3.15722779e-04 1.13596895e-05\n",
      " 2.61660977e-05 1.88981469e-05 3.42182884e-05 2.47951536e-05\n",
      " 6.17833045e-05 1.47638112e-04 1.92172434e-04 5.70230615e-04\n",
      " 1.87847857e-04 2.70032398e-04 3.49173183e-05 9.65605598e-05\n",
      " 5.72970954e-04 1.14543670e-04 1.11651249e-04 1.45799837e-04\n",
      " 2.04068680e-04 2.58866986e-04 1.21541712e-05 3.79650446e-05\n",
      " 6.90795503e-05 1.45800939e-04 3.57509739e-04 5.93817665e-05\n",
      " 9.93027916e-05 7.23343773e-05 2.14089429e-04 1.49053474e-02\n",
      " 1.40832650e-04 2.07088638e-05 2.86684576e-03 5.89497014e-05\n",
      " 3.25954628e-04 7.84255315e-05 1.29038007e-05 1.18673994e-04\n",
      " 3.56821896e-02 2.20762867e-05 4.40691106e-05 2.08162279e-04\n",
      " 4.33224053e-05 1.73988631e-05 4.78040192e-05 1.60356331e-05\n",
      " 1.28864920e-04 3.01216748e-05 4.64714673e-04 7.17148306e-05\n",
      " 1.24523295e-03 1.88617762e-04 3.98634458e-05 1.45120640e-04\n",
      " 2.39487405e-05 8.88574500e-05 3.10799193e-04 1.37382007e-04\n",
      " 9.33933854e-05 1.71438520e-04 1.14043381e-04 1.24519070e-04\n",
      " 4.21481261e-04 2.42168638e-04 7.35339343e-04 1.44068796e-04\n",
      " 9.40017540e-05 8.90892807e-05 1.74506992e-04 9.15084675e-05\n",
      " 2.23630614e-03 6.23605047e-05 1.64067183e-04 3.57633644e-03\n",
      " 5.78603822e-05 4.22132804e-04 3.64308220e-05 1.38094969e-04\n",
      " 8.46491914e-05 7.60178372e-05 1.08883319e-03 1.62067400e-04\n",
      " 5.42022277e-05 2.37518858e-01 1.35737002e-04 2.61703952e-05\n",
      " 1.88717304e-04 5.14693326e-05 8.21850894e-05 2.24547920e-04\n",
      " 3.09169353e-02 1.89201893e-04 5.03111870e-05 9.95516124e-05\n",
      " 3.99102607e-05 1.36629812e-04 1.07682489e-04 1.20551328e-04\n",
      " 7.24108983e-05 2.00473125e-05 2.47316564e-04 1.25010439e-04\n",
      " 1.27262830e-04 1.70983643e-04 2.78632600e-05 2.00016871e-04\n",
      " 1.15907346e-04 3.60612704e-04 1.40721564e-05 7.67512167e-05\n",
      " 1.74279007e-04 2.13509891e-04 2.92890801e-04 2.14021059e-03\n",
      " 7.69531127e-02 3.60138404e-05 2.54372111e-05 3.95080211e-05\n",
      " 8.40836158e-05 1.47304951e-03 2.83579340e-04 7.40473159e-03\n",
      " 4.96868906e-04 4.16213223e-03 9.65763923e-05 2.91512617e-04\n",
      " 4.94430020e-05 1.85361872e-05 2.37226057e-04 8.93182711e-05\n",
      " 6.22700038e-04 1.56019293e-04 1.88883786e-04 8.63419404e-05\n",
      " 7.80122242e-05 2.46525841e-04 2.38192469e-04 1.77614371e-03\n",
      " 3.09014709e-05 3.39269991e-05 9.02372991e-05 1.97157915e-04\n",
      " 3.04832156e-05 2.93976458e-04 9.42913859e-05 3.80603681e-04\n",
      " 1.16177306e-04 1.72518757e-04 3.54317218e-05 1.66584762e-05\n",
      " 4.34642424e-05 8.81092441e-04 6.78076186e-05 5.79212600e-05\n",
      " 1.93113436e-04 5.42029509e-05 6.87070711e-05 2.18718014e-04\n",
      " 6.40968400e-05 9.36967875e-03 1.51172297e-04 7.13585045e-04\n",
      " 4.72803813e-05 5.20333012e-04 1.35678252e-05 5.73275799e-05\n",
      " 1.93396960e-05 3.16615080e-05 2.29389365e-04 1.85742882e-04\n",
      " 4.38372262e-05 4.19102027e-05 3.61772120e-05 1.58708109e-04\n",
      " 9.06836852e-05 8.29231023e-05 2.18745189e-04 7.55783421e-05\n",
      " 3.10135178e-04 1.05798387e-04 2.88176762e-05 5.19619488e-05\n",
      " 1.34733609e-05 5.45249791e-05 3.24434308e-05 2.63605532e-05\n",
      " 1.02498803e-04 5.09824719e-04 1.04161528e-04 7.59014380e-05\n",
      " 1.47620858e-03 1.11330375e-04 1.14738265e-04 1.37528045e-05\n",
      " 3.19482279e-05 9.98912305e-03 8.85651291e-05 3.57807818e-05\n",
      " 1.23875543e-04 1.99341928e-02 2.35898293e-03 7.30975168e-05\n",
      " 4.97888895e-05 1.05719061e-04 8.69685195e-05 1.90806195e-04\n",
      " 6.38932615e-05 5.18464819e-05 1.26880699e-04 2.77860477e-05\n",
      " 7.55249512e-05 1.35967674e-04 1.01854065e-04 5.04540552e-04\n",
      " 4.41077253e-05 4.85983954e-02 1.94869094e-04 4.79257749e-03\n",
      " 2.91980515e-04 2.48168455e-05 1.37170782e-04 4.09175215e-04\n",
      " 5.99772812e-05 1.53196212e-04 8.80657536e-05 1.38201076e-03\n",
      " 1.70422163e-04 1.01421307e-03 1.40214365e-05 4.87680091e-05\n",
      " 8.58224156e-04 3.78273140e-06 1.60147772e-03 2.19177936e-04\n",
      " 1.74519017e-05 1.46342545e-05 8.32898128e-03 3.39671259e-02\n",
      " 3.95724557e-04 1.94406819e-01 2.31950751e-04 8.92770677e-05\n",
      " 4.78123579e-05 1.04466086e-03 2.11110916e-05 1.04199324e-04\n",
      " 2.18912709e-04 1.85953246e-04 2.90342013e-05 8.48196324e-05\n",
      " 5.07808409e-04 2.73517129e-04 8.05893225e-05]\n",
      "[8.30646011e-05 3.40101552e-04 1.14798211e-04 1.28583187e-04\n",
      " 2.03717161e-04 4.63504419e-04 6.71771908e-04 3.46644546e-04\n",
      " 1.57960158e-04 4.83872048e-04 1.62666648e-03 1.57650135e-04\n",
      " 6.00853604e-04 2.64082667e-03 3.07620943e-04 3.19880459e-04\n",
      " 8.24226526e-04 1.44432321e-03 1.23548675e-03 2.04229923e-04\n",
      " 2.52917418e-03 4.59011682e-01 2.23258800e-03 1.91851420e-03\n",
      " 3.31819797e-03 2.34606109e-04 6.52634492e-04 1.70160941e-03\n",
      " 3.20813959e-04 5.81960574e-05 2.77178452e-03 1.32276832e-03\n",
      " 2.67648563e-04 3.30957612e-04 4.50405342e-04 6.56429325e-04\n",
      " 1.03524957e-04 7.60843105e-04 6.41564947e-04 6.98954803e-04\n",
      " 9.90624435e-04 2.56367202e-04 4.52982477e-04 3.67444489e-04\n",
      " 5.15398537e-03 6.65323950e-03 3.76086847e-04 9.20162626e-05\n",
      " 5.01119341e-04 4.41641432e-04 4.41602215e-04 1.46984992e-04\n",
      " 6.04750288e-04 1.45013043e-03 1.46617597e-03 2.94230652e-04\n",
      " 7.82103035e-04 4.44477092e-04 9.30532879e-05 5.30142161e-04\n",
      " 1.08507781e-03 2.93833375e-05 1.26924869e-04 1.21559823e-04\n",
      " 1.36847179e-04 2.08845039e-04 3.75116144e-03 3.20038295e-04\n",
      " 3.30347802e-04 1.27897152e-03 5.10430919e-04 8.85520379e-04\n",
      " 8.37742745e-04 6.59436499e-04 2.61825034e-03 1.54562819e-04\n",
      " 7.81942591e-03 3.32189592e-04 1.19955077e-04 4.04994633e-04\n",
      " 1.05414221e-03 1.20040190e-03 6.86617808e-04 7.62243690e-05\n",
      " 1.27317867e-03 1.56148591e-03 1.34839031e-04 3.50212301e-03\n",
      " 1.52886979e-04 2.72023240e-04 4.42920866e-04 4.80923199e-04\n",
      " 3.97945689e-04 1.62598550e-04]\n",
      "Figure(1000x1000)\n",
      "Random Forest model trained and saved. LFG!\n"
     ]
    }
   ],
   "source": [
    "!python /sietch_colab/akapoor/Demographic_Inference/snakemake_scripts/random_forest_evaluation.py \\\n",
    "    --features_and_targets_filepath /sietch_colab/akapoor/Demographic_Inference/split_isolation_model_dadi_analysis_True_moments_analysis_True_momentsLD_analysis_True_seed_42/sims/sims_pretrain_500_sims_inference_1_seed_42_num_replicates_3_top_values_2/features_and_targets.pkl \\\n",
    "    --model_config_path /sietch_colab/akapoor/Demographic_Inference/model_config.json \\\n",
    "    --color_shades_file /sietch_colab/akapoor/Demographic_Inference/split_isolation_model_dadi_analysis_True_moments_analysis_True_momentsLD_analysis_True_seed_42/sims/sims_pretrain_500_sims_inference_1_seed_42_num_replicates_3_top_values_2/color_shades.pkl \\\n",
    "    --main_colors_file /sietch_colab/akapoor/Demographic_Inference/split_isolation_model_dadi_analysis_True_moments_analysis_True_momentsLD_analysis_True_seed_42/sims/sims_pretrain_500_sims_inference_1_seed_42_num_replicates_3_top_values_2/main_colors.pkl \\\n",
    "    --experiment_config_filepath /sietch_colab/akapoor/Demographic_Inference/experiment_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory created/verified: split_isolation_model_dadi_analysis_True_moments_analysis_True_momentsLD_analysis_True_seed_42/models/sims_pretrain_500_sims_inference_1_seed_42_num_replicates_3_top_values_2/num_hidden_neurons_1000_num_hidden_layers_2_num_epochs_100_dropout_value_0_weight_decay_0_batch_size_64_EarlyStopping_False\n",
      "\n",
      "No XGBoost hyperparameters specified. Running RandomizedSearchCV to find best hyperparameters...\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "Best hyperparameters found via RandomizedSearchCV: {'subsample': 0.6, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 20, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "\n",
      "Initializing XGBRegressor with kwargs={'n_estimators': 500, 'max_depth': 20, 'learning_rate': 0.05, 'subsample': 0.6, 'colsample_bytree': 0.6, 'min_child_weight': 3, 'reg_lambda': 1, 'reg_alpha': 0}\n",
      "\n",
      "XGBoost predictions shape (training): (375, 4)\n",
      "XGBoost predictions shape (validation): (94, 4)\n",
      "\n",
      "[5.30619700e-09 4.81805513e-08 3.15171194e-07 6.23486709e-07\n",
      " 5.25306666e-08 1.53636087e-07 1.95585964e-07 1.61626101e-07\n",
      " 3.13209679e-07 1.39502090e-08 1.02498513e-07 8.89420665e-08\n",
      " 1.50672147e-07 9.34772917e-08 5.01051865e-08 1.14604662e-07\n",
      " 1.84470879e-07 5.33538406e-07 7.33043004e-08 1.83912370e-08\n",
      " 1.93798778e-07 1.91595949e-07 4.79166653e-08 4.31931042e-07\n",
      " 1.87919184e-08 1.24844944e-07 7.01368190e-08 1.84367746e-07\n",
      " 1.03044949e-07 6.41062230e-08 1.44134135e-05 3.79331055e-07\n",
      " 2.88889782e-08 1.17024857e-07 2.70285977e-07 5.98511882e-08\n",
      " 1.13781452e-07 4.51316058e-08 1.13368315e-07 6.83127020e-08\n",
      " 2.93462666e-05 1.17777356e-08 3.56732268e-07 7.68252567e-07\n",
      " 9.53912808e-08 3.27238060e-07 1.20110002e-08 7.42070115e-08\n",
      " 6.40890463e-07 2.63569097e-06 1.03421411e-07 1.33249660e-07\n",
      " 4.15279241e-08 2.72429116e-07 6.63438458e-07 1.88844210e-06\n",
      " 8.03032164e-08 6.08868595e-08 6.76505895e-07 7.69147354e-08\n",
      " 9.04376972e-08 3.73889356e-07 8.84443561e-08 1.62391192e-07\n",
      " 4.19896186e-06 4.78785444e-06 2.83674165e-07 5.76646526e-09\n",
      " 6.93473133e-08 6.03353405e-07 1.93308993e-07 1.77375378e-07\n",
      " 1.97678338e-06 3.17067366e-08 7.72662175e-07 5.28101764e-08\n",
      " 2.19339922e-07 2.83475676e-08 5.38132721e-08 7.41805354e-08\n",
      " 1.23618058e-07 4.84834242e-05 3.26066718e-08 1.23235221e-07\n",
      " 2.81689414e-07 7.83010481e-08 2.17582063e-07 7.15155534e-08\n",
      " 1.42548010e-06 2.34175626e-07 4.04320340e-08 1.13691613e-08\n",
      " 3.42875401e-07 7.16186673e-08 2.11890956e-07 2.04993366e-07\n",
      " 2.12891190e-08 6.47588373e-08 1.57316051e-07 2.86616631e-08\n",
      " 1.33158545e-07 6.47644845e-07 3.50856702e-06 2.64823549e-07\n",
      " 2.00072371e-06 4.21010036e-08 1.15813671e-08 1.22032333e-07\n",
      " 7.64605391e-08 8.84610170e-08 9.27654638e-08 4.41931856e-08\n",
      " 5.08651538e-07 9.66156128e-08 1.49260513e-07 1.85635031e-07\n",
      " 4.02815224e-07 9.65543587e-07 1.37965567e-07 1.68363253e-07\n",
      " 1.49251382e-07 1.38575488e-07 2.81277049e-08 3.18447561e-08\n",
      " 7.52583950e-08 1.06554987e-07 5.03095770e-07 2.25398301e-07\n",
      " 2.53814483e-08 2.92052299e-07 6.16097318e-08 1.53357012e-07\n",
      " 1.12715530e-07 1.65013176e-07 4.49232128e-08 1.28355696e-07\n",
      " 6.79624678e-08 8.32286760e-08 1.18773312e-06 2.57452489e-08\n",
      " 2.69454937e-06 3.72615967e-06 5.12592116e-07 2.61884600e-08\n",
      " 6.70842912e-08 9.72183407e-08 1.37269370e-07 1.84598547e-07\n",
      " 1.39829202e-07 7.71279520e-08 1.62749838e-07 7.98973639e-06\n",
      " 2.20167077e-08 6.29785209e-07 5.41740234e-08 7.56648495e-07\n",
      " 8.84079738e-08 2.05927802e-08 7.21612083e-08 1.31536651e-08\n",
      " 2.97954625e-08 7.82127755e-08 2.65223417e-07 6.00040708e-07\n",
      " 8.29207613e-08 1.84061835e-08 9.83075507e-08 5.83461825e-08\n",
      " 7.57001999e-09 2.36929278e-08 7.47950402e-08 6.49340625e-08\n",
      " 1.91241383e-07 4.15746842e-08 3.45156864e-08 6.45535687e-08\n",
      " 1.76607975e-07 3.31057379e-08 1.50001195e-06 6.80729378e-08\n",
      " 1.10581149e-07 8.25019623e-08 5.96935770e-07 2.63619846e-08\n",
      " 1.29344695e-06 6.22621780e-08 1.60872103e-06 5.73664949e-08\n",
      " 6.63918964e-08 7.11953844e-08 1.18265681e-07 3.66457446e-07\n",
      " 2.69962442e-05 3.16668500e-08 2.27056187e-07 1.02897512e-06\n",
      " 3.16797290e-07 3.00828234e-07 1.19320247e-07 4.04665492e-08\n",
      " 2.24380055e-07 1.35741490e-07 6.67100296e-08 1.12607753e-07\n",
      " 9.88986369e-08 1.37583328e-03 6.73485359e-07 2.37188049e-08\n",
      " 2.12446781e-07 1.85522603e-07 3.72911722e-08 5.10737168e-08\n",
      " 3.91659831e-06 3.08796892e-08 1.12485738e-07 6.76466355e-08\n",
      " 4.57870791e-08 4.32534843e-08 7.35217605e-08 7.79941415e-07\n",
      " 1.01722005e-07 2.97639098e-08 1.10191179e-06 3.75422717e-07\n",
      " 9.69605959e-08 7.86782339e-08 4.18027773e-08 8.66776020e-07\n",
      " 9.29931953e-08 1.04806795e-07 1.63774805e-07 6.63497820e-08\n",
      " 4.28544601e-07 1.85853837e-07 3.45779881e-07 3.87432116e-06\n",
      " 1.81606110e-06 3.19696869e-08 1.37734745e-07 7.10004726e-07\n",
      " 1.08059781e-07 1.24438542e-05 1.74867193e-06 2.26571192e-04\n",
      " 3.03515669e-07 6.05827248e-07 1.19258460e-07 2.87113780e-07\n",
      " 2.12971625e-07 8.20626072e-08 1.83028675e-07 3.74161548e-08\n",
      " 1.00429860e-07 5.76521209e-08 2.32820621e-06 1.96030265e-09\n",
      " 4.35205636e-08 1.62901542e-07 8.70420820e-08 1.36768441e-07\n",
      " 2.42026543e-07 6.25322684e-08 1.25679255e-07 7.19193190e-08\n",
      " 8.71228646e-07 4.29625051e-08 1.39417171e-07 1.67816378e-07\n",
      " 4.50990852e-08 1.65496097e-07 7.01869257e-08 3.32151845e-08\n",
      " 9.08485176e-08 2.92625663e-08 3.73954359e-08 1.56843493e-07\n",
      " 1.66475077e-07 1.86347139e-07 1.06093558e-07 9.72695215e-07\n",
      " 8.31756423e-08 2.04450725e-07 1.20524501e-07 1.28446417e-07\n",
      " 2.68196446e-08 2.52564871e-07 1.11673952e-07 4.14619888e-07\n",
      " 5.12462683e-08 1.50834978e-06 1.32532925e-08 1.23110026e-07\n",
      " 3.54656379e-08 4.40479716e-07 5.00022564e-08 7.90913081e-08\n",
      " 3.04092421e-07 4.39469899e-07 1.14891761e-07 1.01588698e-08\n",
      " 1.80966651e-07 2.27262471e-07 2.55344579e-07 1.13164243e-07\n",
      " 1.49121692e-07 5.85879543e-08 6.31892747e-07 4.23811471e-08\n",
      " 2.93563349e-07 8.61471603e-08 3.90618363e-07 1.74652204e-07\n",
      " 3.16477561e-08 1.34758918e-08 4.13139757e-08 1.31626069e-07\n",
      " 2.05130419e-07 6.40540413e-08 1.16816626e-07 2.70516673e-08\n",
      " 5.61661707e-07 3.46775636e-07 1.37180385e-07 2.47274523e-08\n",
      " 9.73412827e-08 7.59557751e-08 3.44145816e-07 3.80958368e-07\n",
      " 3.37563056e-07 3.84939617e-07 8.73133810e-08 3.59417035e-07\n",
      " 4.81512831e-06 2.21112621e-07 6.03482657e-08 1.67304097e-07\n",
      " 1.05377949e-07 5.89372680e-05 1.72867875e-06 8.26076565e-08\n",
      " 1.28906443e-06 1.36465328e-07 3.50741129e-07 3.29253881e-07\n",
      " 3.41876713e-05 5.86715164e-08 3.73582358e-08 2.10486862e-07\n",
      " 3.94558909e-08 6.16797867e-05 1.49242505e-07 1.01499390e-06\n",
      " 5.06165586e-07 1.67578275e-07 1.80259321e-06 4.70613792e-08\n",
      " 4.05431416e-07 1.81123186e-07 4.89778928e-06 3.78654176e-06\n",
      " 4.75736960e-09 1.59345035e-05 2.24339864e-07 7.68313849e-08\n",
      " 1.69722588e-06 5.04031883e-07 1.62961518e-07 3.78294324e-08\n",
      " 9.92570727e-08 1.98336528e-07 1.40267229e-07 2.86917965e-07\n",
      " 8.93498769e-07 4.43364104e-07 8.72688311e-07]\n",
      "[2.22859167e-04 6.51206745e-04 2.53106759e-04 2.18527388e-04\n",
      " 2.36537271e-04 3.24638600e-04 5.01757963e-04 2.95443637e-04\n",
      " 2.50417892e-04 5.30374208e-04 1.76120975e-04 6.26262068e-04\n",
      " 3.47915004e-04 4.86210229e-03 8.98464941e-04 8.38960331e-04\n",
      " 4.15644013e-03 1.51860092e-03 1.06927736e-03 7.28114201e-04\n",
      " 9.06998842e-04 3.58221812e-01 6.43269122e-04 1.52063384e-03\n",
      " 5.92948065e-03 1.38731629e-04 1.28611876e-03 9.34256833e-04\n",
      " 1.47155094e-03 1.60823859e-04 2.24096791e-03 3.93069528e-03\n",
      " 2.97659314e-04 1.52108771e-04 1.00401023e-03 5.34895592e-03\n",
      " 9.31932308e-05 3.94469806e-04 2.39821166e-04 4.39066627e-04\n",
      " 1.04198110e-04 3.04251115e-04 1.88170529e-03 6.46544248e-04\n",
      " 1.54565477e-03 2.71433151e-03 9.09687187e-04 4.55309546e-04\n",
      " 6.26229289e-04 8.36701342e-04 5.89201811e-04 6.85325956e-04\n",
      " 2.72549836e-04 1.05386265e-03 1.57455163e-03 1.26159031e-04\n",
      " 2.34951952e-03 8.37476772e-04 1.82613809e-04 1.03698396e-03\n",
      " 5.91086962e-04 1.28981692e-04 5.03644524e-04 6.92703884e-04\n",
      " 1.02506014e-03 1.35452086e-04 7.70541860e-04 6.22149895e-04\n",
      " 4.51328102e-04 1.06967348e-04 7.80988084e-04 4.27674652e-04\n",
      " 6.81828136e-04 9.00784428e-04 2.96791004e-03 3.02811876e-04\n",
      " 7.96814471e-03 4.09514591e-04 1.42883318e-03 3.25702127e-04\n",
      " 7.06850275e-04 1.33617130e-03 1.12754366e-03 5.48007019e-04\n",
      " 1.23637540e-03 1.67617588e-03 5.10844649e-04 3.39671682e-04\n",
      " 1.83960800e-03 2.91940766e-04 3.61201458e-04 1.13136594e-03\n",
      " 5.90191104e-04 1.67174103e-04]\n",
      "Figure(1000x1000)\n",
      "XGBoost model trained and saved. LFG!\n"
     ]
    }
   ],
   "source": [
    " !python /sietch_colab/akapoor/Demographic_Inference/snakemake_scripts/xgboost_evaluation.py \\\n",
    "    --features_and_targets_filepath /sietch_colab/akapoor/Demographic_Inference/split_isolation_model_dadi_analysis_True_moments_analysis_True_momentsLD_analysis_True_seed_42/sims/sims_pretrain_500_sims_inference_1_seed_42_num_replicates_3_top_values_2/features_and_targets.pkl \\\n",
    "    --model_config_path /sietch_colab/akapoor/Demographic_Inference/model_config.json \\\n",
    "    --color_shades_file /sietch_colab/akapoor/Demographic_Inference/split_isolation_model_dadi_analysis_True_moments_analysis_True_momentsLD_analysis_True_seed_42/sims/sims_pretrain_500_sims_inference_1_seed_42_num_replicates_3_top_values_2/color_shades.pkl \\\n",
    "    --main_colors_file /sietch_colab/akapoor/Demographic_Inference/split_isolation_model_dadi_analysis_True_moments_analysis_True_momentsLD_analysis_True_seed_42/sims/sims_pretrain_500_sims_inference_1_seed_42_num_replicates_3_top_values_2/main_colors.pkl \\\n",
    "    --experiment_config_filepath /sietch_colab/akapoor/Demographic_Inference/experiment_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # parser.add_argument(\"--experiment_directory\", type=str, required=True)\n",
    "    # parser.add_argument(\"--model_config_file\", type=str, required=True)\n",
    "    # parser.add_argument(\"--features_file\", type=str, required=True)\n",
    "    # parser.add_argument(\"--color_shades\", type=str, required=True)\n",
    "    # parser.add_argument(\"--main_colors\", type=str, required=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Values in the dataset:\n",
      "  Training features max: dadi_rep1_Na                  29463.672544\n",
      "moments_rep1_Na               29856.497542\n",
      "moments_rep1_FIM_element_0        0.254189\n",
      "moments_rep1_FIM_element_1        0.287948\n",
      "moments_rep1_FIM_element_2       17.340964\n",
      "moments_rep1_FIM_element_3        0.653049\n",
      "moments_rep1_FIM_element_4        0.120641\n",
      "moments_rep1_FIM_element_5       18.464149\n",
      "moments_rep1_FIM_element_6        0.658112\n",
      "moments_rep1_FIM_element_7        0.343955\n",
      "moments_rep1_FIM_element_8        2.814908\n",
      "moments_rep1_FIM_element_9        4.545339\n",
      "dadi_rep1_N1                  29554.623419\n",
      "moments_rep1_N1               29885.956435\n",
      "dadi_rep1_N2                  29582.069317\n",
      "moments_rep1_N2               29868.185783\n",
      "dadi_rep1_t_split             19930.419102\n",
      "moments_rep1_t_split          19817.833040\n",
      "dadi_rep2_Na                  29611.360053\n",
      "moments_rep2_Na               29857.481577\n",
      "moments_rep2_FIM_element_0        0.261474\n",
      "moments_rep2_FIM_element_1        0.412274\n",
      "moments_rep2_FIM_element_2       12.982855\n",
      "moments_rep2_FIM_element_3        0.639771\n",
      "moments_rep2_FIM_element_4        0.238458\n",
      "moments_rep2_FIM_element_5       15.179212\n",
      "moments_rep2_FIM_element_6        0.728319\n",
      "moments_rep2_FIM_element_7        0.321204\n",
      "moments_rep2_FIM_element_8        2.793974\n",
      "moments_rep2_FIM_element_9        3.838703\n",
      "dadi_rep2_N1                  29554.591131\n",
      "moments_rep2_N1               29885.133289\n",
      "dadi_rep2_N2                  29572.411020\n",
      "moments_rep2_N2               29821.697841\n",
      "dadi_rep2_t_split             19930.151542\n",
      "moments_rep2_t_split          19818.185758\n",
      "momentsLD_Na                  29920.126699\n",
      "momentsLD_N1                  29766.614224\n",
      "momentsLD_N2                  29981.242416\n",
      "momentsLD_t_split             19978.792013\n",
      "dtype: float64\n",
      "  Training targets max:  simulated_params_Na         1.722017\n",
      "simulated_params_N1         1.728109\n",
      "simulated_params_N2         1.726078\n",
      "simulated_params_t_split    1.718905\n",
      "dtype: float64\n",
      "  Validation features max: dadi_rep1_Na                  29541.985430\n",
      "moments_rep1_Na               29743.152190\n",
      "moments_rep1_FIM_element_0        0.253892\n",
      "moments_rep1_FIM_element_1        0.287145\n",
      "moments_rep1_FIM_element_2        4.318750\n",
      "moments_rep1_FIM_element_3        0.550813\n",
      "moments_rep1_FIM_element_4        0.120600\n",
      "moments_rep1_FIM_element_5        2.334381\n",
      "moments_rep1_FIM_element_6        0.589435\n",
      "moments_rep1_FIM_element_7        0.343416\n",
      "moments_rep1_FIM_element_8        2.074574\n",
      "moments_rep1_FIM_element_9        3.663266\n",
      "dadi_rep1_N1                  29427.267022\n",
      "moments_rep1_N1               29701.018648\n",
      "dadi_rep1_N2                  29689.321445\n",
      "moments_rep1_N2               29960.919308\n",
      "dadi_rep1_t_split             19927.591907\n",
      "moments_rep1_t_split          19505.952204\n",
      "dadi_rep2_Na                  29558.093400\n",
      "moments_rep2_Na               29834.263677\n",
      "moments_rep2_FIM_element_0        0.261120\n",
      "moments_rep2_FIM_element_1        0.410924\n",
      "moments_rep2_FIM_element_2        6.024786\n",
      "moments_rep2_FIM_element_3        0.765699\n",
      "moments_rep2_FIM_element_4        0.238319\n",
      "moments_rep2_FIM_element_5        5.129744\n",
      "moments_rep2_FIM_element_6        0.608353\n",
      "moments_rep2_FIM_element_7        0.320676\n",
      "moments_rep2_FIM_element_8        2.610539\n",
      "moments_rep2_FIM_element_9        2.038254\n",
      "dadi_rep2_N1                  29427.035472\n",
      "moments_rep2_N1               29703.159283\n",
      "dadi_rep2_N2                  29689.312919\n",
      "moments_rep2_N2               29960.564210\n",
      "dadi_rep2_t_split             19927.624566\n",
      "moments_rep2_t_split          19505.479701\n",
      "momentsLD_Na                  29600.756263\n",
      "momentsLD_N1                  29952.248390\n",
      "momentsLD_N2                  29910.641794\n",
      "momentsLD_t_split             19994.032379\n",
      "dtype: float64\n",
      "  Validation targets max:  simulated_params_Na         1.718433\n",
      "simulated_params_N1         1.698963\n",
      "simulated_params_N2         1.723570\n",
      "simulated_params_t_split    1.725478\n",
      "dtype: float64\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | network   | Sequential | 564    | train\n",
      "1 | criterion | MSELoss    | 0      | train\n",
      "-------------------------------------------------\n",
      "564       Trainable params\n",
      "0         Non-trainable params\n",
      "564       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]/home/akapoor/miniconda3/envs/myenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n",
      "/home/akapoor/miniconda3/envs/myenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n",
      "/home/akapoor/miniconda3/envs/myenv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "Epoch 0: 100%|█████████████████████████| 6/6 [00:00<00:00, 58.77it/s, v_num=194]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1061.85it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 908.45it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 6/6 [00:00<00:00, 382.15it/s, v_num=194, val_loss=1.04e+6, trai\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1091.98it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 930.31it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 6/6 [00:00<00:00, 387.16it/s, v_num=194, val_loss=7.95e+5, trai\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1091.41it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 927.64it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 6/6 [00:00<00:00, 384.89it/s, v_num=194, val_loss=6.23e+5, trai\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1097.70it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 914.39it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 6/6 [00:00<00:00, 388.36it/s, v_num=194, val_loss=5.01e+5, trai\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1098.85it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 941.38it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 6/6 [00:00<00:00, 388.72it/s, v_num=194, val_loss=4.12e+5, trai\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1100.58it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 943.81it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 6/6 [00:00<00:00, 390.77it/s, v_num=194, val_loss=3.44e+5, trai\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1097.12it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 940.43it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 6/6 [00:00<00:00, 390.51it/s, v_num=194, val_loss=2.93e+5, trai\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1106.68it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 925.89it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 6/6 [00:00<00:00, 390.51it/s, v_num=194, val_loss=2.53e+5, trai\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1102.60it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 939.69it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 6/6 [00:00<00:00, 391.58it/s, v_num=194, val_loss=2.21e+5, trai\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1094.26it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 940.53it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 6/6 [00:00<00:00, 390.71it/s, v_num=194, val_loss=1.94e+5, tra\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1106.97it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 938.85it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 6/6 [00:00<00:00, 386.83it/s, v_num=194, val_loss=1.73e+5, tra\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1098.85it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 939.90it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 6/6 [00:00<00:00, 390.76it/s, v_num=194, val_loss=1.55e+5, tra\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1105.22it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 940.53it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 6/6 [00:00<00:00, 390.68it/s, v_num=194, val_loss=1.39e+5, tra\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████▌         | 1/2 [00:00<00:00, 885.81it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 850.51it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 6/6 [00:00<00:00, 390.14it/s, v_num=194, val_loss=1.26e+5, tra\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1099.71it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 935.81it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 6/6 [00:00<00:00, 393.08it/s, v_num=194, val_loss=1.15e+5, tra\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1110.19it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 947.76it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 6/6 [00:00<00:00, 390.82it/s, v_num=194, val_loss=1.05e+5, tra\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1106.09it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 948.08it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 6/6 [00:00<00:00, 334.00it/s, v_num=194, val_loss=9.57e+4, tra\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1100.58it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 932.48it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 6/6 [00:00<00:00, 388.74it/s, v_num=194, val_loss=8.8e+4, trai\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1106.38it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 941.91it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 6/6 [00:00<00:00, 392.47it/s, v_num=194, val_loss=8.12e+4, tra\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 1/2 [00:00<00:00, 1108.43it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 2/2 [00:00<00:00, 947.33it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 6/6 [00:00<00:00, 301.97it/s, v_num=194, val_loss=7.51e+4, tra`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "Epoch 19: 100%|█| 6/6 [00:00<00:00, 261.18it/s, v_num=194, val_loss=7.51e+4, tra\n",
      "Figure(1000x1000)\n",
      "[ 16637.36014808  37163.1841446   18879.93207817  86502.08599612\n",
      "  33223.79812615  92111.6856233   78403.99406716  73530.31111534\n",
      "  14983.66074792  39985.22722942  82383.86948848  48268.24260936\n",
      "   5264.8940447  352979.891593    14792.32565287  52890.8344005\n",
      "  14803.7903202  225891.23527018  93376.55912574  10179.66094111\n",
      "  34701.97339231  11780.06320323 151248.95077671 147507.11283382\n",
      "  36304.29449966  86883.56163519  12559.04847794  70994.88652039\n",
      "   1641.47777964   6071.91447111  81473.05700834  69050.60927531\n",
      "  55577.41107639  43446.46226406  50611.80101319  15115.21643105\n",
      " 252182.2827008   20388.23689596 136798.80996664  16878.01857202\n",
      " 110571.44611633  48026.75103396  31840.58759022  47998.00859899\n",
      " 105440.96578404 105478.39398924  12499.65335864  59735.24676165\n",
      " 316436.51690108   4265.22956805  14193.99878202 213630.23808552\n",
      "  43490.07437412 105648.01118504 142675.11208866 115625.05805184\n",
      "  14401.52084771  29992.10439364 190278.59356525  19388.30817563\n",
      "  26502.48059114  40504.79106146   5588.76967037  33240.5638299\n",
      "  56391.29252613  54579.68346801  36264.37290047 166890.9779688\n",
      " 148396.15353584 273241.88090604  16837.08887014 303568.41896321\n",
      "  18269.25726493 125220.92435485 165829.25287253 316401.21595578\n",
      "  36720.23435348  14864.64726321  48239.4781901   58351.13010508\n",
      "  25432.34512253  26070.34233534  37093.93118098  13413.6119529\n",
      "   9778.46036238  94812.58316593 108183.42152152  66276.24473281\n",
      "  50519.74668213  25712.9210295  101733.64620635  61944.88813753\n",
      "  21225.15131001   3043.91596572  11375.36165532  34658.58267424\n",
      " 380331.18609995 127920.68414167  13881.48897233  13948.21155695\n",
      " 309519.23671575   6131.92379095 267913.56825694  17924.01257917\n",
      "  88794.26613219  22620.65326108 139794.72920051 283147.79559584\n",
      " 197213.15166595 116103.09434877  69843.9733478  102205.10579528\n",
      "  68176.75656996   5205.35535248 183724.05033005 153274.78343227\n",
      "   6765.08626692 133335.10870512  28214.95136965  95741.66427972\n",
      " 176017.2365786  163104.02481885  13949.86054389  11009.73134114\n",
      "  21440.6268334   47706.99263746  52328.96668562  47577.7171838\n",
      " 169402.31589131   4234.79852816 105849.91391871  33456.1246841\n",
      "  37804.53940253  34339.39616727  30068.14285987  18978.31044368\n",
      "  28761.23809996  47485.62693563  16315.02389666  12912.62267576\n",
      " 261119.65684996  63493.57923391 136330.59086503  93837.70828726\n",
      " 123526.1607802   38931.24286339  19922.8052824   35620.29688021\n",
      "  45189.31812662  10381.94293598  17597.27935017  79761.3102539\n",
      " 625796.01733074 149198.82730563  32809.68344542  53144.06830916\n",
      "   9379.57456069   8740.00895909  45310.00515815  33781.55613392\n",
      "  16686.78461327  20069.41506837   5312.93069609  68567.08860132\n",
      "  18788.70539039  39658.25126103 182241.8084206   97216.61137132\n",
      "  59270.42109164  29060.27667758  54707.57498421   9366.02028844\n",
      "  84942.11744563   3162.45700192  18602.2285082   66745.50028654\n",
      " 237362.69466797  32979.31933091  29810.20909253  15252.0516264\n",
      " 172329.99566158 314383.8487579   82320.65646299  91755.49235621\n",
      "  66876.54317532 296061.53693099  47139.30853014 122968.21530441\n",
      " 185242.58573784  29224.28080015   4841.51326935  34780.8269329\n",
      " 143403.72364014 103667.13154393  19963.04144891  13457.4853056\n",
      "  22048.65337785  53022.03242813  12699.36676308  67434.97787332\n",
      "  67108.05137502   3382.53856811  84326.78217451  44961.88812935\n",
      "  30213.17191715  47338.68899755  12932.2557127   11927.56376255\n",
      "  45653.59578338  20379.97618087  49758.87208623  70269.62489626\n",
      "   7812.2209687   13810.63748095  12874.69889256 134144.20334395\n",
      "  72985.70708036  45688.719821     5066.4392169   85491.3593783\n",
      "   7434.97294319  35055.13829156  33895.54951298  37946.98382824\n",
      " 252585.0818289   76325.24603763  66572.27224074 202331.13705607\n",
      " 374208.10214355  61287.48567543   7119.32166753  44501.84631707\n",
      " 191534.15202961 146111.87656037  34579.50201363  10770.64950253\n",
      "  87068.70687637  12317.01949249   9513.89360806  31390.31633501\n",
      " 114804.497904   281759.54394913 207150.64294027  21650.8349976\n",
      " 187107.11843592 160663.0790314   13027.71641238 281496.98705732\n",
      "   3678.9493029   33201.01358859  46838.16406184  24617.11150814\n",
      "   9091.14294603   9180.50141554 362160.44442977 210970.0111619\n",
      "  21916.22805866 133709.70152973  36194.18885065  66936.97870408\n",
      "  30199.98268423  11298.77954389  41465.88047342   9306.86563405\n",
      "   8382.98128325  18519.28446541   4554.24399202  28125.42167793\n",
      "   4910.07716768  25852.21632212   3365.37872291 133131.72040526\n",
      "  58596.5857433    7969.99257771  31945.75357985  50245.59469217\n",
      "  50289.81114836  25847.88540096  42269.5872011  270171.90592382\n",
      "  89200.72175293   6895.82901569  21694.8581155   48468.98469577\n",
      " 116943.39871021  71834.80620139  69480.58171504   7749.12155286\n",
      "   3174.92005299 524899.38533766  18227.56776231   8074.08116741\n",
      "  25981.74881015  19285.23988731  17258.67688278  55783.06110435\n",
      "   8221.51966094  47080.46834309  33967.59982965 147689.0927396\n",
      "  19699.79272352  74591.58958273  85982.7031055   39359.74839973\n",
      " 151723.28036803  10065.67277382 118611.17709988 137234.328965\n",
      " 171076.52507049 132261.25987673  45492.46366805 107475.25175062\n",
      "  11390.47721613  28724.70997773 128680.2821552    8690.17751876\n",
      " 155319.66571036  31420.77802958 115328.41587833  23057.02076469\n",
      " 134505.11626629 299527.12692597  17688.9444445   19264.96256565\n",
      "   7045.49359767  52133.3530868  132051.74586608 368822.26299877\n",
      "  29647.15607061  24817.82633177 115301.5963901   46769.75765506\n",
      "  64957.38629568 124854.37085279   5899.73935561  62008.8111124\n",
      "  97108.68970638 153812.49637202  63004.99231411  25006.73672161\n",
      "  25301.04010642 135301.53448678  16810.19208533  31871.87274515\n",
      "  36523.37850173 193723.78735678 180013.4850872   68633.76956545\n",
      " 114111.33292586  52222.02206239  15035.42733716   8858.18327195\n",
      "  27977.14426559  10462.93624765  72455.99399162 239064.12127418\n",
      "  13882.12027265  13355.62985938 114312.89336121 109960.28833275\n",
      "  90803.05480205  28442.66297332  17614.35368831   7607.7593726\n",
      "  70824.54607622 309928.07360818  20331.4364201   21282.87123449\n",
      " 199009.00865132  11111.36384771   1818.90662239  56293.71496086\n",
      "  92100.67809674  58472.43607173  31255.55593904]\n",
      "[  8381.72973748  13703.35362044 218303.93909752  10284.98520185\n",
      "  43800.0209583   17752.00550195 249860.56293498  19875.73692655\n",
      "  30386.2358591   46595.80857979 101143.66869112 118696.53010596\n",
      " 280258.06045803  49811.45368443  13548.10949125  38589.18192103\n",
      "    890.42561688  90050.78485962  33067.35419105  22826.18929961\n",
      "  90613.2278522   46725.20639151  52454.14189135  15883.92139044\n",
      " 107940.49200421   5025.1750743   37096.32363719   5961.06598918\n",
      "   6727.81829118 177071.71226258  87225.87529134   8976.0193844\n",
      " 691243.39182653   7911.61128978 104094.93477444  21980.68198913\n",
      "  36491.8376664   12912.03676248  62569.68030869  15065.01381858\n",
      "  20724.33249624  22700.0217504   20334.66849237  37401.16794228\n",
      " 496060.79541865 244765.23736795  51928.57675085  75705.43795348\n",
      "  89893.67941425 121356.88584827  13495.96990858  30312.26178951\n",
      "  48616.92593712 159100.09115976  38724.5691228   34221.43554695\n",
      "  40266.14113791 161350.5464197    4097.64112681  69032.22025873\n",
      "  67604.73963063  59745.28631935 221212.62829702 342094.30828923\n",
      "  79655.13385533  18822.42726118  29136.69473297  66360.86869989\n",
      "  29533.51399654  20861.29176584  11819.96831631  64975.49187152\n",
      " 131219.65636138  29141.31531457 124037.58307946   6658.0165264\n",
      " 115461.66653733  14670.45751664  61643.24568595  99760.38603859\n",
      " 136401.81304609  19340.08083194  87304.32460661  90065.2582822\n",
      "  31424.31764201  21787.05060399  13588.98792886  10544.80456247\n",
      "   8609.98565719  88974.48369045 108713.47135141  18712.68326049\n",
      "   4805.64262056  47080.18838519]\n"
     ]
    }
   ],
   "source": [
    "!python /sietch_colab/akapoor/Demographic_Inference/snakemake_scripts/setup_trainer.py \\\n",
    "    --experiment_directory $EXPERIMENT_DIRECTORY \\\n",
    "    --model_config_file $MODEL_CONFIG_FILEPATH \\\n",
    "    --features_file $SIM_DIRECTORY/features_and_targets.pkl \\\n",
    "    --color_shades $SIM_DIRECTORY/color_shades.pkl \\\n",
    "    --main_colors $SIM_DIRECTORY/main_colors.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? False\n",
      "Number of GPUs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akapoor/miniforge3/envs/snakemake-env/lib/python3.12/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs CUDA available?\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of GPUs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent GPU:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU Name:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/snakemake-env/lib/python3.12/site-packages/torch/cuda/__init__.py:878\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m~/miniforge3/envs/snakemake-env/lib/python3.12/site-packages/torch/cuda/__init__.py:314\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    313\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 314\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    318\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current GPU:\", torch.cuda.current_device())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
