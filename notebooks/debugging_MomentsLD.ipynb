{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MomentsLD makes me sad :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import moments\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import msprime\n",
    "import dadi\n",
    "import glob\n",
    "import demes\n",
    "import ray\n",
    "import json\n",
    "os.chdir('/sietch_colab/akapoor/Demographic_Inference')\n",
    "import src.demographic_models as demographic_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to see what's going wrong with my MomentsLD specific scripts. I will copy and paste them here and will debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions in the preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_params(lower_bound_params, upper_bound_params):\n",
    "    sampled_params = {}\n",
    "    for key in lower_bound_params:\n",
    "        lower_bound = lower_bound_params[key]\n",
    "        upper_bound = upper_bound_params[key]\n",
    "        sampled_value = np.random.uniform(lower_bound, upper_bound)\n",
    "        sampled_params[key] = int(sampled_value)\n",
    "\n",
    "\n",
    "        # Check if the sampled parameter is equal to the mean of the uniform distribution\n",
    "        mean_value = (lower_bound + upper_bound) / 2\n",
    "        if sampled_value == mean_value:\n",
    "            # Add a small random value to avoid exact mean, while keeping within bounds\n",
    "            adjustment = np.random.uniform(-0.1 * (upper_bound - lower_bound), 0.1 * (upper_bound - lower_bound))\n",
    "            adjusted_value = sampled_value + adjustment\n",
    "            \n",
    "            # Ensure the adjusted value is still within the bounds\n",
    "            adjusted_value = max(min(adjusted_value, upper_bound), lower_bound)\n",
    "            sampled_params[key] = int(adjusted_value)\n",
    "\n",
    "    return sampled_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_msprime_replicates(sampled_params, experiment_config, folderpath):\n",
    "\n",
    "    if experiment_config[\"demographic_model\"] == \"bottleneck_model\":\n",
    "        demographic_model = demographic_models.bottleneck_model\n",
    "\n",
    "    elif experiment_config[\"demographic_model\"] == \"split_isolation_model\":\n",
    "        demographic_model = demographic_models.split_isolation_model_simulation\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported demographic model: {experiment_config['demographic_model']}\")\n",
    "\n",
    "    g = demographic_model(sampled_params)\n",
    "    demog = msprime.Demography.from_demes(g)\n",
    "\n",
    "    # Create directory for storing VCFs\n",
    "    output_folder = folderpath\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Samples: {experiment_config['num_samples']}\")\n",
    "    \n",
    "    tree_sequences = msprime.sim_ancestry(\n",
    "        samples = experiment_config['num_samples'],\n",
    "        # {\"N1\": experiment_config['num_samples']['N1'], \"N2\": experiment_config['num_samples']['N2']},\n",
    "        demography=demog,\n",
    "        sequence_length=experiment_config['genome_length'],\n",
    "        recombination_rate=experiment_config['recombination_rate'],\n",
    "        num_replicates=experiment_config['num_reps'],\n",
    "        random_seed=experiment_config['seed'],\n",
    "    )\n",
    "\n",
    "    # List to store file paths of the generated VCFs\n",
    "    vcf_filepaths = []\n",
    "\n",
    "    for ii, ts in enumerate(tree_sequences):\n",
    "        ts = msprime.sim_mutations(ts, rate=experiment_config['mutation_rate'], random_seed=ii + 1)\n",
    "        vcf_name = os.path.join(output_folder, f'rep.{ii}.vcf')\n",
    "        with open(vcf_name, \"w+\") as fout:\n",
    "            ts.write_vcf(fout, allow_position_zero=True)\n",
    "        os.system(f\"gzip {vcf_name}\")\n",
    "\n",
    "        # Store the compressed VCF file path\n",
    "        vcf_filepaths.append(f\"{vcf_name}.gz\")\n",
    "\n",
    "    # Write the metadata file with all VCF file paths\n",
    "    metadata_file = os.path.join(output_folder, \"metadata.txt\")\n",
    "    with open(metadata_file, \"w+\") as metafile:\n",
    "        metafile.write(\"\\n\".join(vcf_filepaths))\n",
    "\n",
    "    print(f\"Metadata file written to {metadata_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_samples_and_rec_map(experiment_config, folderpath):\n",
    "\n",
    "    # Define the file paths\n",
    "    samples_file = os.path.join(folderpath, f\"samples.txt\")\n",
    "    flat_map_file = os.path.join(folderpath, f\"flat_map.txt\")\n",
    "\n",
    "    print(f'Samples filepath: {samples_file}')\n",
    "    print(f'Flat map filepath: {flat_map_file}')\n",
    "\n",
    "    # Open and write the sample file\n",
    "    with open(samples_file, \"w+\") as fout:\n",
    "        fout.write(\"sample\\tpop\\n\")\n",
    "\n",
    "        # Dynamically define samples based on the num_samples dictionary\n",
    "        sample_idx = 0  # Initialize sample index\n",
    "        for pop_name, sample_size in experiment_config['num_samples'].items():\n",
    "            for _ in range(sample_size):\n",
    "                fout.write(f\"tsk_{sample_idx}\\t{pop_name}\\n\")\n",
    "                sample_idx += 1\n",
    "\n",
    "    # Write the recombination map file\n",
    "    with open(flat_map_file, \"w+\") as fout:\n",
    "        fout.write(\"pos\\tMap(cM)\\n\")\n",
    "        fout.write(\"0\\t0\\n\")\n",
    "        fout.write(f\"{experiment_config['genome_length']}\\t{experiment_config['recombination_rate'] * experiment_config['genome_length'] * 100}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions in the demographic_models module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_isolation_model_simulation(sampled_params):\n",
    "\n",
    "    # Unpack the sampled parameters\n",
    "    Na, N1, N2, m, t_split = (\n",
    "        sampled_params[\"Na\"],  # Effective population size of the ancestral population\n",
    "        sampled_params[\"N1\"],  # Size of population 1 after split\n",
    "        sampled_params[\"N2\"],  # Size of population 2 after split\n",
    "        sampled_params[\"m\"],   # Migration rate between populations\n",
    "        sampled_params[\"t_split\"],  # Time of the population split (in generations)\n",
    "    )\n",
    "\n",
    "    b = demes.Builder()\n",
    "    b.add_deme(\"Na\", epochs=[dict(start_size=Na, end_time=t_split)])\n",
    "    b.add_deme(\"N1\", ancestors=[\"Na\"], epochs=[dict(start_size=N1)])\n",
    "    b.add_deme(\"N2\", ancestors=[\"Na\"], epochs=[dict(start_size=N2)])\n",
    "    b.add_migration(demes=[\"N1\", \"N2\"], rate=m)\n",
    "    g = b.resolve()\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for the MomentsLD inference part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your function with Ray's remote decorator\n",
    "@ray.remote\n",
    "def get_LD_stats(vcf_file, r_bins, flat_map_path, pop_file_path):\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    ld_stats = moments.LD.Parsing.compute_ld_statistics( #type:ignore\n",
    "        vcf_file,\n",
    "        rec_map_file=flat_map_path,\n",
    "        pop_file=pop_file_path,\n",
    "        pops=[\"N1\", \"N2\"], # TODO: Change later\n",
    "        r_bins=r_bins,\n",
    "        report=False,\n",
    "    )\n",
    "\n",
    "    return ld_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ld_stats_parallel(folderpath, num_reps, r_bins):\n",
    "    \n",
    "    flat_map_path = os.path.join(folderpath, \"flat_map.txt\")\n",
    "    pop_file_path = os.path.join(folderpath, \"samples.txt\")\n",
    "    vcf_files = [\n",
    "        os.path.join(folderpath, f\"rep.{rep_ii}.vcf.gz\")\n",
    "        for rep_ii in range(num_reps)\n",
    "    ]\n",
    "\n",
    "    # Launch the tasks in parallel using Ray\n",
    "    futures = [\n",
    "        get_LD_stats.remote(vcf_file, r_bins, flat_map_path, pop_file_path)\n",
    "        for vcf_file in vcf_files\n",
    "    ]\n",
    "\n",
    "    # Wait for all the tasks to complete and retrieve results\n",
    "    results = ray.get(futures)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_momentsLD(folderpath, demographic_model, p_guess, num_reps):\n",
    "    \"\"\"\n",
    "    This should do the parameter inference for momentsLD\n",
    "    index: unique simulation number\n",
    "    \"\"\"\n",
    "\n",
    "    r_bins = np.array([0, 1e-6, 2e-6, 5e-6, 1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3])\n",
    "\n",
    "    print(\"parsing LD statistics\")\n",
    "\n",
    "\n",
    "    ld_stats = {}\n",
    "    results = compute_ld_stats_parallel(folderpath, num_reps, r_bins)\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        ld_stats[i] = result\n",
    "\n",
    "    # print(\"computing mean and varcov matrix from LD statistics sums\")\n",
    "    mv = moments.LD.Parsing.bootstrap_data(ld_stats)  # type: ignore\n",
    "    # print(\"SHAPE OF THE COVARIANCE MATRIX\")\n",
    "    # print(mv[\"varcovs\"][-1].shape)\n",
    "    # mv[\"varcovs\"][-1].shape = (1, 1)\n",
    "\n",
    "    if demographic_model == \"bottleneck_model\":\n",
    "        demo_func = moments.LD.Demographics1D.three_epoch # type: ignore\n",
    "\n",
    "    elif demographic_model == \"split_isolation_model\":\n",
    "        demo_func = demographic_models.split_isolation_model_momentsLD\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported demographic model: {demographic_model}\")\n",
    "\n",
    "    # Set up the initial guess\n",
    "    p_guess = moments.LD.Util.perturb_params(p_guess, fold=0.1) # type: ignore\n",
    "    opt_params, LL = moments.LD.Inference.optimize_log_lbfgsb( #type:ignore\n",
    "        p_guess, [mv[\"means\"], mv[\"varcovs\"]], [demo_func], rs=r_bins, maxiter = 100, verbose = 3\n",
    "    )\n",
    "\n",
    "    physical_units = moments.LD.Util.rescale_params( # type: ignore\n",
    "        opt_params, [\"nu\", \"nu\", \"T\", \"m\", \"Ne\"]\n",
    ")\n",
    "\n",
    "    opt_params, LL = moments.LD.Inference.optimize_log_lbfgsb( #type:ignore\n",
    "    p_guess, [mv[\"means\"], mv[\"varcovs\"]], [demo_func], rs=r_bins, verbose = 3\n",
    "    )\n",
    "\n",
    "    opt_params_dict = {}\n",
    "    if demographic_model == \"bottleneck_model\":\n",
    "\n",
    "        opt_params_dict = {\n",
    "            # \"N0\": opt_params[4],\n",
    "            \"Nb\": opt_params[0] * opt_params[4],\n",
    "            \"N_recover\": opt_params[1] * opt_params[4],\n",
    "            \"t_bottleneck_start\": (opt_params[2]+opt_params[3]) * 2 * opt_params[4],\n",
    "            \"t_bottleneck_end\": opt_params[3] * 2 * opt_params[4]\n",
    "        }\n",
    "\n",
    "    elif demographic_model == \"split_isolation_model\":\n",
    "        physical_units = moments.LD.Util.rescale_params( #type:ignore\n",
    "            opt_params, [\"nu\", \"nu\", \"T\", \"m\", \"Ne\"]\n",
    "        )\n",
    "\n",
    "        print(physical_units)\n",
    "\n",
    "        opt_params_dict = {\n",
    "            \"N1\": physical_units[0],\n",
    "            \"N2\": physical_units[1],\n",
    "            \"t_split\": physical_units[2],\n",
    "            \"m\": physical_units[3], \n",
    "            'Na': physical_units[4]\n",
    "        }\n",
    "\n",
    "        print(\"best fit parameters:\")\n",
    "        print(f\"  N(deme0)         :  {physical_units[0]:.1f}\")\n",
    "        print(f\"  N(deme1)         :  {physical_units[1]:.1f}\")\n",
    "        print(f\"  Div. time (gen)  :  {physical_units[2]:.1f}\")\n",
    "        print(f\"  Migration rate   :  {physical_units[3]:.6f}\")\n",
    "        print(f\"  N(ancestral)     :  {physical_units[4]:.1f}\")\n",
    "    \n",
    "    # print(f'Moments LD results: {opt_params_dict}')\n",
    "\n",
    "    return opt_params_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the experiment_config.json\n",
    "with open(\"/sietch_colab/akapoor/Demographic_Inference/experiment_config.json\", \"r\") as f:\n",
    "    experiment_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_params = sample_params(experiment_config[\"lower_bound_params\"], experiment_config[\"upper_bound_params\"])\n",
    "print(sampled_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = f\"/sietch_colab/akapoor/Demographic_Inference/testing_things/simulations/{experiment_config['demographic_model']}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_msprime_replicates(sampled_params, experiment_config, folderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_samples_and_rec_map(experiment_config, folderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_model = \"split_isolation_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_guess = p_guess = [0.1, 2, 0.075, 2, 10000]\n",
    "num_reps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params_dict = run_inference_momentsLD(folderpath, demographic_model, p_guess, num_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dadi makes me sad :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import moments\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import msprime\n",
    "import dadi\n",
    "import glob\n",
    "import demes\n",
    "import ray\n",
    "import json\n",
    "os.chdir('/sietch_colab/akapoor/Demographic_Inference')\n",
    "import src.demographic_models as demographic_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the experiment_config.json\n",
    "with open(\"/sietch_colab/akapoor/Demographic_Inference/experiment_config.json\", \"r\") as f:\n",
    "    experiment_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_params(lower_bound_params, upper_bound_params):\n",
    "    sampled_params = {}\n",
    "    for key in lower_bound_params:\n",
    "        lower_bound = lower_bound_params[key]\n",
    "        upper_bound = upper_bound_params[key]\n",
    "        sampled_value = np.random.uniform(lower_bound, upper_bound)\n",
    "        sampled_params[key] = int(sampled_value)\n",
    "\n",
    "\n",
    "        # Check if the sampled parameter is equal to the mean of the uniform distribution\n",
    "        mean_value = (lower_bound + upper_bound) / 2\n",
    "        if sampled_value == mean_value:\n",
    "            # Add a small random value to avoid exact mean, while keeping within bounds\n",
    "            adjustment = np.random.uniform(-0.1 * (upper_bound - lower_bound), 0.1 * (upper_bound - lower_bound))\n",
    "            adjusted_value = sampled_value + adjustment\n",
    "            \n",
    "            # Ensure the adjusted value is still within the bounds\n",
    "            adjusted_value = max(min(adjusted_value, upper_bound), lower_bound)\n",
    "            sampled_params[key] = int(adjusted_value)\n",
    "\n",
    "    return sampled_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t_split': 4782, 'N1': 5104, 'N2': 6980, 'Na': 18976, 'm': 0}\n"
     ]
    }
   ],
   "source": [
    "sampled_params = sample_params(experiment_config[\"lower_bound_params\"], experiment_config[\"upper_bound_params\"])\n",
    "print(sampled_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_SFS(\n",
    "    experiment_config, sampled_params, mode, num_samples, demographic_model, length=1e7, mutation_rate=5.7e-9, recombination_rate = 3.386e-9, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    If we are in pretraining mode we will use a simulated SFS. If we are in inference mode we will use a real SFS.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if mode == \"pretrain\":\n",
    "        # Simulate the demographic model\n",
    "        g = demographic_model(sampled_params)\n",
    "        demog = msprime.Demography.from_demes(g)\n",
    "\n",
    "        # Dynamically define the samples using msprime.SampleSet, based on the sample_sizes dictionary\n",
    "        samples = [\n",
    "            msprime.SampleSet(sample_size, population=pop_name, ploidy=1)\n",
    "            for pop_name, sample_size in num_samples.items()\n",
    "        ]\n",
    "\n",
    "        # Simulate ancestry for two populations (joint simulation)\n",
    "        ts = msprime.sim_ancestry(\n",
    "            samples=samples,  # Two populations\n",
    "            demography=demog,\n",
    "            sequence_length=length,\n",
    "            recombination_rate=recombination_rate,\n",
    "            random_seed=experiment_config['seed'],\n",
    "        )\n",
    "        \n",
    "        # Simulate mutations over the ancestry tree sequence\n",
    "        ts = msprime.sim_mutations(ts, rate=mutation_rate)\n",
    "\n",
    "        # Define sample sets dynamically for the SFS\n",
    "        sample_sets = [\n",
    "            ts.samples(population=pop.id) \n",
    "            for pop in ts.populations() \n",
    "            if len(ts.samples(population=pop.id)) > 0  # Exclude populations with no samples\n",
    "        ]\n",
    "        \n",
    "        # Create the joint allele frequency spectrum\n",
    "        sfs = ts.allele_frequency_spectrum(sample_sets=sample_sets, mode=\"site\", polarised=True)\n",
    "        \n",
    "        # Multiply SFS by the sequence length to adjust scale\n",
    "        sfs *= length\n",
    "\n",
    "        # Convert to moments Spectrum for further use\n",
    "        sfs = moments.Spectrum(sfs)\n",
    "    \n",
    "    elif mode == \"inference\":\n",
    "        vcf_file = kwargs.get(\"vcf_file\", None)\n",
    "        pop_file = kwargs.get(\"pop_file\", None)\n",
    "        popname = kwargs.get(\"popname\", None)\n",
    "\n",
    "        if vcf_file is None or pop_file is None:\n",
    "            raise ValueError(\n",
    "                \"vcf_file and pop_file must be provided in inference mode.\"\n",
    "            )\n",
    "\n",
    "        dd = dadi.Misc.make_data_dict_vcf(vcf_file, pop_file)\n",
    "        sfs = dadi.Spectrum.from_data_dict(\n",
    "            dd, [popname], projections=[2 * num_samples], polarized=True\n",
    "        )\n",
    "\n",
    "    return sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = create_SFS(\n",
    "    experiment_config,\n",
    "      sampled_params,\n",
    "        \"pretrain\",\n",
    "          experiment_config[\"num_samples\"],\n",
    "            demographic_models.split_isolation_model_simulation,\n",
    "              length=experiment_config['genome_length'],\n",
    "                mutation_rate=experiment_config['mutation_rate'], recombination_rate = experiment_config['recombination_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spectrum([[-- 447.0 203.0 152.0 132.0 79.0 72.0 48.0 42.0 32.0 34.0 24.0 18.0 19.0\n",
       "  37.0 23.0 1.0 3.0 11.0 8.0 1.0]\n",
       " [356.0 7.0 12.0 28.0 1.0 12.0 11.0 19.0 4.0 3.0 10.0 5.0 0.0 5.0 2.0 0.0\n",
       "  0.0 3.0 3.0 1.0 1.0]\n",
       " [210.0 11.0 15.0 14.0 6.0 4.0 14.0 2.0 16.0 6.0 2.0 4.0 3.0 9.0 3.0 4.0\n",
       "  1.0 0.0 3.0 5.0 0.0]\n",
       " [115.0 6.0 7.0 7.0 9.0 10.0 4.0 5.0 8.0 1.0 1.0 6.0 5.0 0.0 6.0 2.0 1.0\n",
       "  0.0 4.0 2.0 1.0]\n",
       " [84.0 4.0 2.0 2.0 15.0 6.0 0.0 2.0 7.0 3.0 8.0 7.0 5.0 2.0 6.0 1.0 0.0\n",
       "  1.0 2.0 5.0 1.0]\n",
       " [59.0 1.0 1.0 3.0 4.0 3.0 6.0 1.0 1.0 0.0 1.0 7.0 0.0 0.0 1.0 2.0 2.0\n",
       "  1.0 0.0 2.0 4.0]\n",
       " [59.0 3.0 3.0 6.0 3.0 8.0 6.0 2.0 1.0 3.0 2.0 1.0 7.0 3.0 2.0 1.0 2.0\n",
       "  1.0 0.0 1.0 5.0]\n",
       " [31.0 5.0 0.0 6.0 2.0 7.0 3.0 3.0 2.0 4.0 4.0 2.0 1.0 5.0 5.0 6.0 0.0\n",
       "  0.0 0.0 1.0 0.0]\n",
       " [42.0 11.0 1.0 2.0 2.0 2.0 6.0 5.0 2.0 1.0 4.0 1.0 0.0 3.0 1.0 3.0 3.0\n",
       "  2.0 3.0 0.0 1.0]\n",
       " [39.0 5.0 2.0 8.0 5.0 2.0 0.0 0.0 4.0 1.0 2.0 3.0 0.0 0.0 5.0 0.0 5.0\n",
       "  0.0 0.0 1.0 16.0]\n",
       " [45.0 4.0 1.0 10.0 3.0 6.0 9.0 1.0 1.0 4.0 13.0 5.0 0.0 0.0 5.0 2.0 2.0\n",
       "  9.0 0.0 0.0 5.0]\n",
       " [42.0 3.0 0.0 4.0 3.0 2.0 1.0 0.0 2.0 1.0 1.0 4.0 0.0 0.0 4.0 3.0 1.0\n",
       "  10.0 0.0 1.0 7.0]\n",
       " [17.0 0.0 3.0 4.0 2.0 3.0 2.0 2.0 3.0 1.0 2.0 4.0 5.0 2.0 4.0 0.0 1.0\n",
       "  1.0 4.0 4.0 4.0]\n",
       " [9.0 5.0 0.0 0.0 3.0 5.0 4.0 4.0 2.0 1.0 6.0 11.0 1.0 0.0 4.0 1.0 0.0\n",
       "  0.0 0.0 2.0 1.0]\n",
       " [13.0 4.0 1.0 2.0 1.0 5.0 8.0 9.0 3.0 0.0 0.0 4.0 2.0 4.0 7.0 4.0 4.0\n",
       "  3.0 2.0 0.0 7.0]\n",
       " [14.0 2.0 5.0 0.0 2.0 4.0 2.0 3.0 0.0 4.0 2.0 3.0 1.0 10.0 4.0 3.0 2.0\n",
       "  0.0 0.0 0.0 8.0]\n",
       " [14.0 7.0 4.0 2.0 0.0 8.0 1.0 1.0 2.0 3.0 3.0 1.0 3.0 1.0 0.0 0.0 9.0\n",
       "  2.0 2.0 5.0 10.0]\n",
       " [10.0 12.0 4.0 1.0 3.0 5.0 4.0 0.0 3.0 11.0 0.0 0.0 8.0 2.0 3.0 1.0 7.0\n",
       "  0.0 6.0 2.0 14.0]\n",
       " [3.0 0.0 4.0 0.0 0.0 2.0 4.0 11.0 0.0 3.0 2.0 10.0 2.0 6.0 5.0 5.0 2.0\n",
       "  1.0 3.0 4.0 16.0]\n",
       " [2.0 2.0 2.0 1.0 2.0 1.0 1.0 2.0 1.0 6.0 7.0 0.0 5.0 6.0 12.0 5.0 2.0\n",
       "  4.0 2.0 10.0 13.0]\n",
       " [1.0 5.0 1.0 0.0 0.0 12.0 21.0 2.0 9.0 7.0 13.0 3.0 3.0 10.0 9.0 8.0\n",
       "  13.0 23.0 12.0 19.0 --]], folded=False, pop_ids=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.parameter_inference import run_inference_dadi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3       , -6934.29    , array([ 0.0147655  ,  1.09256    ,  0.047487   ,  0.02723    ])\n",
      "6       , -14927.7    , array([ 0.00376638 ,  0.624363   ,  0.047487   ,  0.02723    ])\n",
      "9       , -6771.8     , array([ 0.0147655  ,  0.624363   ,  0.047487   ,  0.00680751 ])\n",
      "OPT DADI PARAMETER: [0.01476551 0.62436314 0.01194675 0.02723004]\n",
      "12      , -9353.06    , array([ 0.0173614  ,  1.42488    ,  0.0940947  ,  0.0530843  ])\n",
      "15      , -11356.1    , array([ 0.00996368 ,  1.42488    ,  0.0940947  ,  0.0928976  ])\n",
      "18      , -6053.52    , array([ 0.00996368 ,  1.42488    ,  0.0235987  ,  0.0530843  ])\n",
      "OPT DADI PARAMETER: [0.00996368 1.4248795  0.02359867 0.05308435]\n",
      "21      , -12150.6    , array([ 0.0124891  ,  1.08654    ,  0.129034   ,  0.0302475  ])\n",
      "24      , -12227.6    , array([ 0.0124891  ,  1.08654    ,  0.225735   ,  0.0302475  ])\n",
      "27      , -11565.7    , array([ 0.0124891  ,  0.271709   ,  0.129034   ,  0.0302475  ])\n",
      "30      , -10542      , array([ 0.0205984  ,  0.944363   ,  0.0001     ,  0.0373044  ])\n",
      "OPT DADI PARAMETER: [0.01248908 1.08653787 0.03233358 0.03024748]\n"
     ]
    }
   ],
   "source": [
    "model_sfs_dadi, opt_theta_dadi, opt_params_dict_dadi, ll_list_dadi = (\n",
    "        run_inference_dadi(\n",
    "            sfs = sfs,\n",
    "            p0= experiment_config['optimization_initial_guess'],\n",
    "            lower_bound= experiment_config['lower_bound_optimization'],\n",
    "            upper_bound= experiment_config['upper_bound_optimization'],\n",
    "            num_samples=20,\n",
    "            demographic_model=experiment_config['demographic_model'],\n",
    "            mutation_rate=experiment_config['mutation_rate'],\n",
    "            length=experiment_config['genome_length'],\n",
    "            k  = experiment_config['k'], \n",
    "            top_values_k = experiment_config['top_values_k']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
