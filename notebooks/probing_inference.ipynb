{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot of raw moments/dadi against simulated params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4923 momentsLD files\n",
      "Found 4923 overlapping pairs\n",
      "\n",
      "Sample of overlapping simulation numbers:\n",
      "Simulation 0\n",
      "Simulation 1\n",
      "Simulation 2\n",
      "Simulation 3\n",
      "Simulation 4\n"
     ]
    }
   ],
   "source": [
    "def get_overlapping_simulations(software_dir, momentsLD_dir):\n",
    "    \"\"\"\n",
    "    Find overlapping simulation files between software and momentsLD directories\n",
    "    \n",
    "    Args:\n",
    "        software_dir (str): Path to software inferences directory\n",
    "        momentsLD_dir (str): Path to momentsLD inferences directory\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Lists of overlapping software and momentsLD files (with full paths)\n",
    "    \"\"\"\n",
    "    # Get list of files in each directory\n",
    "    momentsLD_files = os.listdir(momentsLD_dir)\n",
    "    software_files = os.listdir(software_dir)\n",
    "    \n",
    "    # Extract numbers from momentsLD files\n",
    "    momentsLD_numbers = set()\n",
    "    momentsLD_file_dict = {}  # Store mapping of number to full filename\n",
    "    \n",
    "    for file in momentsLD_files:\n",
    "        if file.startswith('momentsLD_inferences_sim_') and file.endswith('.pkl'):\n",
    "            try:\n",
    "                num = int(file.split('_')[-1].split('.')[0])\n",
    "                momentsLD_numbers.add(num)\n",
    "                momentsLD_file_dict[num] = file\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Find matching software files\n",
    "    overlapping_software_files = []\n",
    "    overlapping_momentsLD_files = []\n",
    "    \n",
    "    for file in software_files:\n",
    "        if file.startswith('software_inferences_sim_') and file.endswith('.pkl'):\n",
    "            try:\n",
    "                num = int(file.split('_')[-1].split('.')[0])\n",
    "                if num in momentsLD_numbers:\n",
    "                    # Store full paths\n",
    "                    overlapping_software_files.append(os.path.join(software_dir, file))\n",
    "                    overlapping_momentsLD_files.append(os.path.join(momentsLD_dir, momentsLD_file_dict[num]))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Sort both lists by simulation number\n",
    "    sorted_pairs = sorted(zip(overlapping_software_files, overlapping_momentsLD_files), \n",
    "                         key=lambda x: int(x[0].split('_')[-1].split('.')[0]))\n",
    "    \n",
    "    overlapping_software_files, overlapping_momentsLD_files = zip(*sorted_pairs)\n",
    "    \n",
    "    print(f\"Found {len(momentsLD_numbers)} momentsLD files\")\n",
    "    print(f\"Found {len(overlapping_software_files)} overlapping pairs\")\n",
    "    \n",
    "    if overlapping_software_files:\n",
    "        print(\"\\nSample of overlapping simulation numbers:\")\n",
    "        for software_file in list(overlapping_software_files)[:5]:\n",
    "            num = int(software_file.split('_')[-1].split('.')[0])\n",
    "            print(f\"Simulation {num}\")\n",
    "            \n",
    "    return list(overlapping_software_files), list(overlapping_momentsLD_files)\n",
    "\n",
    "# Use the function\n",
    "software_inferences_dir = \"/sietch_colab/akapoor/Demographic_Inference/software_inferences_dir\"\n",
    "momentsLD_inferences_dir = \"/sietch_colab/akapoor/Demographic_Inference/final_LD_inferences\"\n",
    "\n",
    "# Get the overlapping files with full paths\n",
    "software_files, momentsLD_files = get_overlapping_simulations(software_inferences_dir, momentsLD_inferences_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(software_files[0], 'rb') as f:\n",
    "    software_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Na': 23876.280308249967,\n",
       "  'N1': 24649.63175921235,\n",
       "  'N2': 14357.349884380737,\n",
       "  't_split': 17159.112138441425,\n",
       "  'm': 184.39996895868322,\n",
       "  'upper_triangular_FIM': array([-4.45638571e+05, -9.07620137e+04,  6.67789909e+05, -3.50415609e+05,\n",
       "         -1.22600390e+06,  1.37925430e+06, -6.01616301e+05, -3.90933710e+06,\n",
       "          2.01353442e+06,  1.24911311e+08]),\n",
       "  'll': 7106.4181658968155},\n",
       " {'Na': 23870.675780510344,\n",
       "  'N1': 24641.84156521614,\n",
       "  'N2': 14365.933378706419,\n",
       "  't_split': 17168.876655566513,\n",
       "  'm': 191.3479371661034,\n",
       "  'upper_triangular_FIM': array([-4.42215289e+05, -9.14434040e+04,  6.68562058e+05, -2.25093321e+05,\n",
       "         -1.21882177e+06,  1.37915113e+06, -5.79152827e+05, -3.87425377e+06,\n",
       "          1.93840776e+06,  1.15950327e+08]),\n",
       "  'll': 7106.414255299631}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "software_data['opt_params_moments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shapes:\n",
      "Predictions shape: (3826, 30)\n",
      "Targets shape: (3826, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "experiment_config_file = '/sietch_colab/akapoor/Demographic_Inference/experiment_config.json'\n",
    "software_inferences_dir = \"/sietch_colab/akapoor/Demographic_Inference/software_inferences_dir\"\n",
    "momentsLD_inferences_dir = \"/sietch_colab/akapoor/Demographic_Inference/final_LD_inferences\"\n",
    "\n",
    "# Load configuration\n",
    "with open(experiment_config_file, \"r\") as f:\n",
    "    experiment_config = json.load(f)\n",
    "\n",
    "parameters = [\"Na\", \"N1\", \"N2\", \"t_split\"]\n",
    "replicates = experiment_config['top_values_k']\n",
    "\n",
    "# Containers for predictions and targets\n",
    "software_predictions_data = []\n",
    "momentsLD_predictions_data = []\n",
    "targets_data = []\n",
    "\n",
    "# Process software inference files\n",
    "for idx, filepath in enumerate(software_files):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        sim_data = pickle.load(f)\n",
    "    \n",
    "    row = {}\n",
    "    \n",
    "    # Verify dadi and moments predictions\n",
    "    for replicate in range(1, replicates + 1):\n",
    "        for param in parameters:\n",
    "            dadi_val = sim_data['opt_params_dadi'][replicate - 1][param]\n",
    "            moments_val = sim_data['opt_params_moments'][replicate - 1][param]\n",
    "\n",
    "            row[f\"dadi_rep{replicate}_{param}\"] = dadi_val\n",
    "            row[f\"moments_rep{replicate}_{param}\"] = moments_val\n",
    "\n",
    "        # Extract FIM elements as separate columns if present\n",
    "        if 'upper_triangular_FIM' in sim_data['opt_params_moments'][0]:\n",
    "            fim = sim_data['opt_params_moments'][replicate-1]['upper_triangular_FIM']\n",
    "            for i, fim_val in enumerate(fim):\n",
    "                row[f\"FIM_element_{i}\"] = fim_val\n",
    "\n",
    "    software_predictions_data.append(row)\n",
    "    targets_data.append({f\"simulated_params_{param}\": sim_data['simulated_params'][param] \n",
    "                        for param in parameters})\n",
    "\n",
    "# Process MomentsLD inference files\n",
    "for idx, filepath in enumerate(momentsLD_files):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        momentsLD_sim_data = pickle.load(f)\n",
    "\n",
    "    row = {}\n",
    "\n",
    "    for param in parameters:\n",
    "        val = momentsLD_sim_data['opt_params_momentsLD'][0][param]\n",
    "        if np.isnan(val):\n",
    "            print(f\"nan value for {param} in {filepath}\")\n",
    "        row[f\"momentsLD_{param}\"] = val\n",
    "\n",
    "    momentsLD_predictions_data.append(row)\n",
    "\n",
    "# Create DataFrames\n",
    "software_df = pd.DataFrame(software_predictions_data)\n",
    "momentsLD_df = pd.DataFrame(momentsLD_predictions_data)\n",
    "targets_df = pd.DataFrame(targets_data)\n",
    "\n",
    "# Combine software and momentsLD predictions\n",
    "combined_predictions_df = pd.concat([software_df, momentsLD_df], axis=1)\n",
    "\n",
    "# Drop any row that has at least one NaN value\n",
    "combined_predictions_df = combined_predictions_df.dropna()\n",
    "valid_indices = combined_predictions_df.dropna().index\n",
    "combined_predictions_df = combined_predictions_df.loc[valid_indices].reset_index(drop=True)\n",
    "targets_df = targets_df.loc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "# Filter based on bounds for all methods and parameters\n",
    "mask = pd.Series(True, index=combined_predictions_df.index)\n",
    "\n",
    "# Filter for each parameter and method\n",
    "methods = ['momentsLD', 'dadi_rep1', 'dadi_rep2', 'moments_rep1', 'moments_rep2']\n",
    "\n",
    "for param in parameters:\n",
    "    # Get bounds for this parameter\n",
    "    lower = experiment_config['lower_bound_params'][param]\n",
    "    upper = experiment_config['upper_bound_params'][param]\n",
    "    \n",
    "    # Add to mask for each method\n",
    "    for method in methods:\n",
    "        col_name = f\"{method}_{param}\"\n",
    "        param_mask = (combined_predictions_df[col_name] >= lower) & (combined_predictions_df[col_name] <= upper)\n",
    "        mask &= param_mask\n",
    "\n",
    "# Apply final mask to both dataframes\n",
    "combined_predictions_df = combined_predictions_df[mask].reset_index(drop=True)\n",
    "targets_df = targets_df[mask].reset_index(drop=True)\n",
    "\n",
    "# Final NaN check\n",
    "combined_predictions_df = combined_predictions_df.dropna()\n",
    "valid_indices = combined_predictions_df.dropna().index\n",
    "combined_predictions_df = combined_predictions_df.loc[valid_indices].reset_index(drop=True)\n",
    "targets_df = targets_df.loc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Final shapes:\")\n",
    "print(f\"Predictions shape: {combined_predictions_df.shape}\")\n",
    "print(f\"Targets shape: {targets_df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train/validation split indices\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(combined_predictions_df)), \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create the preprocessing results object\n",
    "preprocessing_results_obj = {\n",
    "    \"training\": {\n",
    "        \"predictions\": combined_predictions_df.iloc[train_indices].reset_index(drop=True),\n",
    "        \"targets\": targets_df.iloc[train_indices].reset_index(drop=True),\n",
    "        \"indices\": train_indices,\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"predictions\": combined_predictions_df.iloc[val_indices].reset_index(drop=True),\n",
    "        \"targets\": targets_df.iloc[val_indices].reset_index(drop=True),\n",
    "        \"indices\": val_indices,\n",
    "    },\n",
    "    \"parameter_names\": parameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_results_obj['training']['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_results_obj['validation']['predictions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_results_obj['training']['predictions'].to_csv('/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2/training_features.csv', index=False)\n",
    "preprocessing_results_obj['training']['targets'].to_csv('/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2/training_targets.csv', index=False)\n",
    "preprocessing_results_obj['validation']['predictions'].to_csv('/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2/validation_features.csv', index=False)\n",
    "preprocessing_results_obj['validation']['targets'].to_csv('/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2/validation_targets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define parameters\n",
    "parameters = ['Na', 'N1', 'N2', 't_split']\n",
    "\n",
    "# Extract ground truth and predictions\n",
    "ground_truth = preprocessing_results_obj['training']['targets']\n",
    "predictions = preprocessing_results_obj['training']['predictions']\n",
    "\n",
    "# Function to calculate MAPE\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Create 2x2 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10), sharey=True)\n",
    "fig.suptitle('Predicted vs Ground Truth for Parameters', fontsize=16)\n",
    "\n",
    "# Flatten axes for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, param in enumerate(parameters):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Extract ground truth for this parameter\n",
    "    ground_truth_param = ground_truth[f'simulated_params_{param}']\n",
    "    \n",
    "    # Extract predictions for each method\n",
    "    moments_rep1 = predictions[f'moments_rep1_{param}']\n",
    "    dadi_rep1 = predictions[f'dadi_rep1_{param}']\n",
    "    momentsLD_param = predictions[f'momentsLD_{param}']\n",
    "    \n",
    "    # Calculate R²\n",
    "    r2_moments = np.corrcoef(ground_truth_param, moments_rep1)[0, 1] ** 2\n",
    "    r2_dadi = np.corrcoef(ground_truth_param, dadi_rep1)[0, 1] ** 2\n",
    "    r2_momentsLD = np.corrcoef(ground_truth_param, momentsLD_param)[0, 1] ** 2\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape_moments = calculate_mape(ground_truth_param, moments_rep1)\n",
    "    mape_dadi = calculate_mape(ground_truth_param, dadi_rep1)\n",
    "    mape_momentsLD = calculate_mape(ground_truth_param, momentsLD_param)\n",
    "\n",
    "    # Plot predictions\n",
    "    ax.scatter(ground_truth_param, moments_rep1, color='red', alpha=0.5, \n",
    "               label=f'Moments MAPE: {mape_moments:.2f}%')\n",
    "    ax.scatter(ground_truth_param, dadi_rep1, color='blue', alpha=0.5, \n",
    "               label=f'Dadi MAPE: {mape_dadi:.2f}%')\n",
    "    ax.scatter(ground_truth_param, momentsLD_param, color='green', alpha=0.5, \n",
    "               label=f'MomentsLD MAPE: {mape_momentsLD:.2f}%')\n",
    "\n",
    "    # Perfect prediction line\n",
    "    ax.plot([ground_truth_param.min(), ground_truth_param.max()],\n",
    "            [ground_truth_param.min(), ground_truth_param.max()],\n",
    "            color='black', linestyle='--', label='Perfect Prediction')\n",
    "    \n",
    "    ax.set_title(f'{param}')\n",
    "    ax.set_xlabel(f'Ground Truth {param}')\n",
    "    if i % 2 == 0:\n",
    "        ax.set_ylabel('Predicted Value')\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust for suptitle\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/sietch_colab/akapoor/Demographic_Inference/')\n",
    "from snakemake_scripts.postprocessing import postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '/sietch_colab/akapoor/Demographic_Inference/experiment_config.json'\n",
    "training_features_filepath = '/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2/training_features.csv'\n",
    "training_targets_filepath = '/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2/training_targets.csv'\n",
    "validation_features_filepath = '/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2/validation_features.csv'\n",
    "validation_targets_filepath = '/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2/validation_targets.csv'\n",
    "\n",
    "# Run the postprocessing function\n",
    "postprocessing_dict = postprocessing(\n",
    "    config_file,\n",
    "    training_features_filepath,\n",
    "    training_targets_filepath,\n",
    "    validation_features_filepath,\n",
    "    validation_targets_filepath\n",
    ")\n",
    "\n",
    "# Save the postprocessing results\n",
    "with open('/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2/postprocessing_results.pkl', 'wb') as f:\n",
    "    pickle.dump(postprocessing_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing_dict['training']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing_dict['training']['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAPE and standard errors for each parameter and method\n",
    "params = ['N1', 'N2', 'Na', 't_split']\n",
    "methods = ['Moments', 'Dadi', 'MomentsLD']\n",
    "mape_values = {}\n",
    "mape_errors = {}\n",
    "\n",
    "for param in params:\n",
    "    mape_values[param] = {}\n",
    "    mape_errors[param] = {}\n",
    "    target_col = f'simulated_params_{param}'\n",
    "    target_values = postprocessing_dict['training']['targets'][target_col]\n",
    "    \n",
    "    for method in methods:\n",
    "        if method == 'Moments':\n",
    "            # Pool predictions from both replicates\n",
    "            predictions = np.concatenate([\n",
    "                postprocessing_dict['training']['predictions'][f'moments_rep1_{param}'],\n",
    "                postprocessing_dict['training']['predictions'][f'moments_rep2_{param}']\n",
    "            ])\n",
    "            # Repeat targets for both replicates\n",
    "            targets = np.tile(target_values, 2)\n",
    "            \n",
    "        elif method == 'Dadi':\n",
    "            # Pool predictions from both replicates\n",
    "            predictions = np.concatenate([\n",
    "                postprocessing_dict['training']['predictions'][f'dadi_rep1_{param}'],\n",
    "                postprocessing_dict['training']['predictions'][f'dadi_rep2_{param}']\n",
    "            ])\n",
    "            # Repeat targets for both replicates\n",
    "            targets = np.tile(target_values, 2)\n",
    "            \n",
    "        else:  # MomentsLD\n",
    "            predictions = postprocessing_dict['training']['predictions'][f'momentsLD_{param}']\n",
    "            targets = target_values\n",
    "\n",
    "        # Calculate absolute percentage error for each simulation\n",
    "        percentage_errors = np.abs((predictions - targets) / targets) * 100\n",
    "        \n",
    "        # Mean MAPE across all simulations\n",
    "        mape_values[param][method] = np.mean(percentage_errors)\n",
    "        \n",
    "        # Standard error of MAPE across simulations\n",
    "        mape_errors[param][method] = np.std(percentage_errors) / np.sqrt(len(percentage_errors))\n",
    "\n",
    "# Create DataFrame\n",
    "result_df = pd.DataFrame({\n",
    "    'Moments': [mape_values[p]['Moments'] for p in params],\n",
    "    'Moments_std_err': [mape_errors[p]['Moments'] for p in params],\n",
    "    'Dadi': [mape_values[p]['Dadi'] for p in params],\n",
    "    'Dadi_std_err': [mape_errors[p]['Dadi'] for p in params],\n",
    "    'MomentsLD': [mape_values[p]['MomentsLD'] for p in params],\n",
    "    'MomentsLD_std_err': [mape_errors[p]['MomentsLD'] for p in params]\n",
    "}, index=params)\n",
    "\n",
    "print(\"MAPE Values and Standard Errors:\")\n",
    "print(result_df)\n",
    "\n",
    "# Create grouped bar plot\n",
    "x = np.arange(len(params))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create bars with error bars\n",
    "rects1 = ax.bar(x - width, [mape_values[p]['Moments'] for p in params], width, \n",
    "              yerr=[mape_errors[p]['Moments'] for p in params],\n",
    "              label='Moments', capsize=5)\n",
    "rects2 = ax.bar(x, [mape_values[p]['Dadi'] for p in params], width,\n",
    "              yerr=[mape_errors[p]['Dadi'] for p in params],\n",
    "              label='Dadi', capsize=5)\n",
    "rects3 = ax.bar(x + width, [mape_values[p]['MomentsLD'] for p in params], width,\n",
    "              yerr=[mape_errors[p]['MomentsLD'] for p in params],\n",
    "              label='MomentsLD', capsize=5)\n",
    "\n",
    "ax.set_ylabel('MAPE (%)')\n",
    "ax.set_title('Mean Absolute Percentage Error by Parameter and Method')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(params)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2/mape_values_preprocess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(result_df, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_the_features(postprocessing_results_filepath, sim_directory):\n",
    "\n",
    "    with open(postprocessing_results_filepath, \"rb\") as file:\n",
    "        postprocessing_results_obj = pickle.load(file)\n",
    "\n",
    "    print(postprocessing_results_obj.keys())\n",
    "\n",
    "    features = {\n",
    "        \"training\": {\"features\": postprocessing_results_obj['training']['predictions'], \"targets\": postprocessing_results_obj['training']['normalized_targets']},\n",
    "        \"validation\": {\"features\": postprocessing_results_obj['validation']['predictions'], \"targets\": postprocessing_results_obj['validation']['normalized_targets']},\n",
    "    }\n",
    "\n",
    "    print(f'Training features shape: {features[\"training\"][\"features\"].shape}')\n",
    "    print(f'Validation features shape: {features[\"validation\"][\"features\"].shape}')\n",
    "\n",
    "    print(f'Training targets shape: {features[\"training\"][\"targets\"].shape}')\n",
    "    print(f'Validation targets shape: {features[\"validation\"][\"targets\"].shape}')\n",
    "\n",
    "\n",
    "    # Now save the dictionary as a pickle\n",
    "    with open(f\"{sim_directory}/features_and_targets.pkl\", \"wb\") as file:\n",
    "        pickle.dump(features, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing_results_filepath = '/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2/postprocessing_results.pkl'\n",
    "sim_directory = '/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getting_the_features(postprocessing_results_filepath, sim_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/sietch_colab/akapoor/Demographic_Inference/sims_pretrain_5000_sims_inference_1_seed_42_num_replicates_3_top_values_2/features_and_targets.pkl', \"rb\") as file:\n",
    "    features = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['training']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
