Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job                  count
-----------------  -------
get_features             1
train_and_predict        1
total                    2

Select jobs to execute...
Execute 1 jobs...

[Mon Sep  2 14:41:11 2024]
localrule get_features:
    input: experiments/new_jawn/preprocessing_results_obj.pkl
    output: experiments/new_jawn/features_and_targets.pkl
    jobid: 0
    reason: Missing output files: experiments/new_jawn/features_and_targets.pkl
    resources: tmpdir=/tmp

[Mon Sep  2 14:41:11 2024]
Finished job 0.
1 of 2 steps (50%) done
Select jobs to execute...
Execute 1 jobs...

[Mon Sep  2 14:41:11 2024]
localrule train_and_predict:
    input: experiments/new_jawn/model_config.json, experiments/new_jawn/features_and_targets.pkl
    output: experiments/new_jawn/snn_results.pkl, experiments/new_jawn/snn_model.pth
    jobid: 3
    reason: Input files updated by another job: experiments/new_jawn/features_and_targets.pkl
    resources: tmpdir=/tmp

[Mon Sep  2 14:41:19 2024]
Finished job 3.
2 of 2 steps (100%) done
Complete log: .snakemake/log/2024-09-02T144111.140817.snakemake.log
